
Starting sweeps

Launching task 0
wandb: Starting wandb agent üïµÔ∏è
2022-01-10 23:29:09,309 - wandb.wandb_agent - INFO - Running runs: []
2022-01-10 23:29:09,710 - wandb.wandb_agent - INFO - Agent received command: run
2022-01-10 23:29:09,710 - wandb.wandb_agent - INFO - Agent starting run with config:
	factor: 0.7042552951616566
	hidden: 83
	hidden_activation: ReLU
	layer_norm: False
	lr: 0.03284403288671747
	nb_edge_layers: 3
	nb_graph_iters: 9
	nb_node_layers: 3
	patience: 12
2022-01-10 23:29:09,713 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --factor=0.7042552951616566 --hidden=83 --hidden_activation=ReLU --layer_norm=False --lr=0.03284403288671747 --nb_edge_layers=3 --nb_graph_iters=9 --nb_node_layers=3 --patience=12
Running main
Mon Jan 10 23:29:14 2022
2022-01-10 23:29:14,727 - wandb.wandb_agent - INFO - Running runs: ['kj5jp0id']
wandb: Currently logged in as: murnanedaniel (use `wandb login --relogin` to force relogin)
wandb: WARNING Ignored wandb.init() arg project when running a sweep
2022-01-10 23:29:16.286649: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
wandb: Tracking run with wandb version 0.12.9
wandb: Syncing run tough-sweep-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/murnanedaniel/HPO_Demo
wandb: üßπ View sweep at https://wandb.ai/murnanedaniel/HPO_Demo/sweeps/4h53uskw
wandb: üöÄ View run at https://wandb.ai/murnanedaniel/HPO_Demo/runs/kj5jp0id
wandb: Run data is saved locally in /global/u2/d/danieltm/ATLAS-Meeting-HPO-Demo/scripts/wandb/run-20220110_232914-kj5jp0id
wandb: Run `wandb offline` to turn off syncing.

Initialising model
Mon Jan 10 23:29:21 2022
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/pytorch_lightning/loggers/wandb.py:342: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse"
[34m[1mwandb[0m: logging graph, to disable use `wandb.watch(log_graph=False)`
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Setting up dataset
Loading events
Events loaded!
Events processed!
Loading events
Events loaded!
Events processed!
Loading events
Events loaded!
Events processed!
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34m[1mwandb[0m: [33mWARNING[0m Config item 'factor' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'hidden' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'hidden_activation' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'layer_norm' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'lr' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'nb_edge_layers' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'nb_graph_iters' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'nb_node_layers' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'patience' was locked by 'sweep' (ignored update).
Set SLURM handle signals.

  | Name                   | Type       | Params
------------------------------------------------------
0 | node_encoder           | Sequential | 14.6 K
1 | edge_encoder           | Sequential | 28.1 K
2 | edge_network           | Sequential | 35.0 K
3 | node_network           | Sequential | 28.1 K
4 | output_edge_classifier | Sequential | 35.3 K
------------------------------------------------------
141 K     Trainable params
0         Non-trainable params
141 K     Total params
0.565     Total estimated model params size (MB)
Validation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0% 0/2 [00:00<?, ?it/s]Validation sanity check:  50% 1/2 [00:00<00:00,  3.79it/s]                                                          Training: 0it [00:00, ?it/s]/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:112: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  f"The dataloader, {name}, does not have many workers which may be a bottleneck."
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 5. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  "Trying to infer the `batch_size` from an ambiguous collection. The batch size we"
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:112: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  f"The dataloader, {name}, does not have many workers which may be a bottleneck."
Training:   0% 0/125 [00:00<?, ?it/s]Epoch 0:   0% 0/125 [00:00<?, ?it/s] Epoch 0:   1% 1/125 [00:00<00:07, 16.81it/s, loss=0.941, v_num=p0id]Epoch 0:   2% 2/125 [00:00<00:06, 17.79it/s, loss=0.941, v_num=p0id]Epoch 0:   2% 2/125 [00:00<00:06, 17.75it/s, loss=0.949, v_num=p0id]Epoch 0:   2% 3/125 [00:00<00:09, 13.12it/s, loss=0.942, v_num=p0id]Epoch 0:   3% 4/125 [00:00<00:08, 15.11it/s, loss=0.942, v_num=p0id]Epoch 0:   3% 4/125 [00:00<00:08, 15.10it/s, loss=0.949, v_num=p0id]Epoch 0:   4% 5/125 [00:00<00:07, 15.80it/s, loss=0.944, v_num=p0id]Epoch 0:   5% 6/125 [00:00<00:07, 16.50it/s, loss=0.941, v_num=p0id]Epoch 0:   6% 7/125 [00:00<00:07, 16.68it/s, loss=0.941, v_num=p0id]Epoch 0:   6% 7/125 [00:00<00:07, 16.67it/s, loss=0.932, v_num=p0id]Epoch 0:   6% 8/125 [00:00<00:06, 17.32it/s, loss=0.94, v_num=p0id] Epoch 0:   7% 9/125 [00:00<00:06, 17.67it/s, loss=0.934, v_num=p0id]Epoch 0:   8% 10/125 [00:00<00:08, 14.00it/s, loss=0.934, v_num=p0id]Epoch 0:   8% 10/125 [00:00<00:08, 13.99it/s, loss=0.933, v_num=p0id]Epoch 0:   9% 11/125 [00:00<00:08, 14.10it/s, loss=0.93, v_num=p0id] Epoch 0:  10% 12/125 [00:00<00:08, 14.00it/s, loss=0.929, v_num=p0id]Epoch 0:  10% 13/125 [00:00<00:07, 14.23it/s, loss=0.929, v_num=p0id]Epoch 0:  10% 13/125 [00:00<00:07, 14.23it/s, loss=0.927, v_num=p0id]Epoch 0:  11% 14/125 [00:00<00:07, 14.45it/s, loss=0.926, v_num=p0id]Epoch 0:  12% 15/125 [00:01<00:07, 14.77it/s, loss=0.922, v_num=p0id]Epoch 0:  13% 16/125 [00:01<00:07, 15.07it/s, loss=0.922, v_num=p0id]Epoch 0:  13% 16/125 [00:01<00:07, 15.06it/s, loss=0.92, v_num=p0id] Epoch 0:  14% 17/125 [00:01<00:07, 15.35it/s, loss=0.917, v_num=p0id]Epoch 0:  14% 18/125 [00:01<00:06, 15.57it/s, loss=0.915, v_num=p0id]Epoch 0:  15% 19/125 [00:01<00:06, 15.70it/s, loss=0.915, v_num=p0id]Epoch 0:  15% 19/125 [00:01<00:06, 15.70it/s, loss=0.912, v_num=p0id]Epoch 0:  16% 20/125 [00:01<00:06, 15.87it/s, loss=0.911, v_num=p0id]Epoch 0:  17% 21/125 [00:01<00:06, 15.98it/s, loss=0.907, v_num=p0id]Epoch 0:  18% 22/125 [00:01<00:06, 16.11it/s, loss=0.907, v_num=p0id]Epoch 0:  18% 22/125 [00:01<00:06, 16.11it/s, loss=0.903, v_num=p0id]Epoch 0:  18% 23/125 [00:01<00:06, 16.31it/s, loss=0.901, v_num=p0id]Epoch 0:  19% 24/125 [00:01<00:06, 16.47it/s, loss=0.896, v_num=p0id]Epoch 0:  20% 25/125 [00:01<00:05, 16.67it/s, loss=0.896, v_num=p0id]Epoch 0:  20% 25/125 [00:01<00:05, 16.67it/s, loss=0.893, v_num=p0id]Epoch 0:  21% 26/125 [00:01<00:05, 16.87it/s, loss=0.889, v_num=p0id]Epoch 0:  22% 27/125 [00:01<00:05, 17.03it/s, loss=0.889, v_num=p0id]Epoch 0:  22% 28/125 [00:01<00:05, 17.11it/s, loss=0.889, v_num=p0id]Epoch 0:  22% 28/125 [00:01<00:05, 17.11it/s, loss=0.883, v_num=p0id]Epoch 0:  23% 29/125 [00:01<00:05, 17.27it/s, loss=0.882, v_num=p0id]Epoch 0:  24% 30/125 [00:01<00:05, 17.39it/s, loss=0.879, v_num=p0id]Epoch 0:  25% 31/125 [00:01<00:05, 17.58it/s, loss=0.879, v_num=p0id]Epoch 0:  25% 31/125 [00:01<00:05, 17.58it/s, loss=0.879, v_num=p0id]Epoch 0:  26% 32/125 [00:01<00:05, 17.66it/s, loss=0.879, v_num=p0id]Epoch 0:  26% 33/125 [00:01<00:05, 17.75it/s, loss=0.878, v_num=p0id]Epoch 0:  27% 34/125 [00:01<00:05, 17.83it/s, loss=0.878, v_num=p0id]Epoch 0:  27% 34/125 [00:01<00:05, 17.82it/s, loss=0.876, v_num=p0id]Epoch 0:  28% 35/125 [00:01<00:05, 17.91it/s, loss=0.876, v_num=p0id]Epoch 0:  29% 36/125 [00:01<00:04, 18.04it/s, loss=0.875, v_num=p0id]Epoch 0:  30% 37/125 [00:02<00:04, 18.25it/s, loss=0.875, v_num=p0id]Epoch 0:  30% 37/125 [00:02<00:04, 18.24it/s, loss=0.875, v_num=p0id]Epoch 0:  30% 38/125 [00:02<00:04, 18.30it/s, loss=0.874, v_num=p0id]Epoch 0:  31% 39/125 [00:02<00:04, 18.38it/s, loss=0.876, v_num=p0id]Epoch 0:  32% 40/125 [00:02<00:04, 18.53it/s, loss=0.876, v_num=p0id]Epoch 0:  32% 40/125 [00:02<00:04, 18.53it/s, loss=0.877, v_num=p0id]Epoch 0:  33% 41/125 [00:02<00:04, 18.63it/s, loss=0.877, v_num=p0id]Epoch 0:  34% 42/125 [00:02<00:04, 18.71it/s, loss=0.876, v_num=p0id]Epoch 0:  34% 43/125 [00:02<00:04, 18.77it/s, loss=0.876, v_num=p0id]Epoch 0:  34% 43/125 [00:02<00:04, 18.77it/s, loss=0.875, v_num=p0id]Epoch 0:  35% 44/125 [00:02<00:04, 18.81it/s, loss=0.877, v_num=p0id]Epoch 0:  36% 45/125 [00:02<00:04, 18.86it/s, loss=0.877, v_num=p0id]Epoch 0:  37% 46/125 [00:02<00:04, 18.90it/s, loss=0.877, v_num=p0id]Epoch 0:  37% 46/125 [00:02<00:04, 18.90it/s, loss=0.877, v_num=p0id]Epoch 0:  38% 47/125 [00:02<00:04, 19.00it/s, loss=0.877, v_num=p0id]Epoch 0:  38% 48/125 [00:02<00:04, 19.00it/s, loss=0.875, v_num=p0id]Epoch 0:  39% 49/125 [00:02<00:03, 19.00it/s, loss=0.875, v_num=p0id]Epoch 0:  39% 49/125 [00:02<00:04, 19.00it/s, loss=0.873, v_num=p0id]Epoch 0:  40% 50/125 [00:02<00:03, 19.03it/s, loss=0.872, v_num=p0id]Epoch 0:  41% 51/125 [00:02<00:03, 19.08it/s, loss=0.869, v_num=p0id]Epoch 0:  42% 52/125 [00:02<00:03, 18.94it/s, loss=0.869, v_num=p0id]Epoch 0:  42% 52/125 [00:02<00:03, 18.94it/s, loss=0.866, v_num=p0id]Epoch 0:  42% 53/125 [00:02<00:03, 18.91it/s, loss=0.865, v_num=p0id]Epoch 0:  43% 54/125 [00:02<00:03, 18.99it/s, loss=0.865, v_num=p0id]Epoch 0:  44% 55/125 [00:02<00:03, 19.00it/s, loss=0.865, v_num=p0id]Epoch 0:  44% 55/125 [00:02<00:03, 19.00it/s, loss=0.863, v_num=p0id]Epoch 0:  45% 56/125 [00:02<00:03, 19.05it/s, loss=0.863, v_num=p0id]Epoch 0:  46% 57/125 [00:02<00:03, 19.15it/s, loss=0.862, v_num=p0id]Epoch 0:  46% 58/125 [00:03<00:03, 19.22it/s, loss=0.862, v_num=p0id]Epoch 0:  46% 58/125 [00:03<00:03, 19.22it/s, loss=0.862, v_num=p0id]Epoch 0:  47% 59/125 [00:03<00:03, 19.22it/s, loss=0.859, v_num=p0id]Epoch 0:  48% 60/125 [00:03<00:03, 19.22it/s, loss=0.857, v_num=p0id]Epoch 0:  49% 61/125 [00:03<00:03, 19.26it/s, loss=0.857, v_num=p0id]Epoch 0:  49% 61/125 [00:03<00:03, 19.26it/s, loss=0.856, v_num=p0id]Epoch 0:  50% 62/125 [00:03<00:03, 19.30it/s, loss=0.856, v_num=p0id]Epoch 0:  50% 63/125 [00:03<00:03, 19.37it/s, loss=0.855, v_num=p0id]Epoch 0:  51% 64/125 [00:03<00:03, 19.41it/s, loss=0.855, v_num=p0id]Epoch 0:  51% 64/125 [00:03<00:03, 19.41it/s, loss=0.856, v_num=p0id]Epoch 0:  52% 65/125 [00:03<00:03, 19.42it/s, loss=0.859, v_num=p0id]Epoch 0:  53% 66/125 [00:03<00:03, 19.46it/s, loss=0.861, v_num=p0id]Epoch 0:  54% 67/125 [00:03<00:02, 19.46it/s, loss=0.861, v_num=p0id]Epoch 0:  54% 67/125 [00:03<00:02, 19.46it/s, loss=0.862, v_num=p0id]Epoch 0:  54% 68/125 [00:03<00:02, 19.48it/s, loss=0.866, v_num=p0id]Epoch 0:  55% 69/125 [00:03<00:02, 19.52it/s, loss=0.87, v_num=p0id] Epoch 0:  56% 70/125 [00:03<00:02, 19.49it/s, loss=0.87, v_num=p0id]Epoch 0:  56% 70/125 [00:03<00:02, 19.49it/s, loss=0.871, v_num=p0id]Epoch 0:  57% 71/125 [00:03<00:02, 19.50it/s, loss=0.873, v_num=p0id]Epoch 0:  58% 72/125 [00:03<00:02, 19.47it/s, loss=0.874, v_num=p0id]Epoch 0:  58% 73/125 [00:03<00:02, 19.47it/s, loss=0.874, v_num=p0id]Epoch 0:  58% 73/125 [00:03<00:02, 19.47it/s, loss=0.876, v_num=p0id]Epoch 0:  59% 74/125 [00:03<00:02, 19.46it/s, loss=0.876, v_num=p0id]Epoch 0:  60% 75/125 [00:03<00:02, 19.48it/s, loss=0.879, v_num=p0id]Epoch 0:  61% 76/125 [00:03<00:02, 19.46it/s, loss=0.879, v_num=p0id]Epoch 0:  61% 76/125 [00:03<00:02, 19.46it/s, loss=0.879, v_num=p0id]Epoch 0:  62% 77/125 [00:03<00:02, 19.40it/s, loss=0.878, v_num=p0id]Epoch 0:  62% 78/125 [00:04<00:02, 19.28it/s, loss=0.88, v_num=p0id] Epoch 0:  63% 79/125 [00:04<00:02, 19.29it/s, loss=0.88, v_num=p0id]Epoch 0:  63% 79/125 [00:04<00:02, 19.28it/s, loss=0.881, v_num=p0id]Epoch 0:  64% 80/125 [00:04<00:02, 19.19it/s, loss=0.88, v_num=p0id] Epoch 0:  65% 81/125 [00:04<00:02, 19.23it/s, loss=0.882, v_num=p0id]Epoch 0:  66% 82/125 [00:04<00:02, 19.28it/s, loss=0.882, v_num=p0id]Epoch 0:  66% 82/125 [00:04<00:02, 19.28it/s, loss=0.884, v_num=p0id]Epoch 0:  66% 83/125 [00:04<00:02, 19.25it/s, loss=0.884, v_num=p0id]Epoch 0:  67% 84/125 [00:04<00:02, 19.24it/s, loss=0.881, v_num=p0id]Epoch 0:  68% 85/125 [00:04<00:02, 19.27it/s, loss=0.881, v_num=p0id]Epoch 0:  68% 85/125 [00:04<00:02, 19.27it/s, loss=0.878, v_num=p0id]Epoch 0:  69% 86/125 [00:04<00:02, 19.26it/s, loss=0.876, v_num=p0id]Epoch 0:  70% 87/125 [00:04<00:01, 19.29it/s, loss=0.877, v_num=p0id]Epoch 0:  70% 88/125 [00:04<00:01, 19.28it/s, loss=0.877, v_num=p0id]Epoch 0:  70% 88/125 [00:04<00:01, 19.28it/s, loss=0.873, v_num=p0id]Epoch 0:  71% 89/125 [00:04<00:01, 19.30it/s, loss=0.871, v_num=p0id]Epoch 0:  72% 90/125 [00:04<00:01, 19.33it/s, loss=0.87, v_num=p0id] Epoch 0:  73% 91/125 [00:04<00:01, 19.31it/s, loss=0.87, v_num=p0id]Epoch 0:  73% 91/125 [00:04<00:01, 19.31it/s, loss=0.869, v_num=p0id]Epoch 0:  74% 92/125 [00:04<00:01, 19.14it/s, loss=0.866, v_num=p0id]Epoch 0:  74% 93/125 [00:04<00:01, 19.09it/s, loss=0.864, v_num=p0id]Epoch 0:  75% 94/125 [00:05<00:01, 18.72it/s, loss=0.864, v_num=p0id]Epoch 0:  75% 94/125 [00:05<00:01, 18.72it/s, loss=0.865, v_num=p0id]Epoch 0:  76% 95/125 [00:05<00:01, 18.76it/s, loss=0.863, v_num=p0id]Epoch 0:  77% 96/125 [00:05<00:01, 18.79it/s, loss=0.861, v_num=p0id]Epoch 0:  78% 97/125 [00:05<00:01, 18.78it/s, loss=0.861, v_num=p0id]Epoch 0:  78% 97/125 [00:05<00:01, 18.78it/s, loss=0.86, v_num=p0id] Epoch 0:  78% 98/125 [00:05<00:01, 18.75it/s, loss=0.859, v_num=p0id]Epoch 0:  79% 99/125 [00:05<00:01, 18.73it/s, loss=0.857, v_num=p0id]Epoch 0:  80% 100/125 [00:05<00:01, 18.65it/s, loss=0.857, v_num=p0id]Epoch 0:  80% 100/125 [00:05<00:01, 18.65it/s, loss=0.857, v_num=p0id]
Validating: 0it [00:00, ?it/s][A
Validating:   0% 0/25 [00:00<?, ?it/s][A
Validating:  24% 6/25 [00:00<00:00, 52.72it/s][AEpoch 0:  86% 107/125 [00:05<00:00, 19.53it/s, loss=0.857, v_num=p0id]
Validating:  48% 12/25 [00:00<00:00, 50.72it/s][AEpoch 0:  91% 114/125 [00:05<00:00, 20.25it/s, loss=0.857, v_num=p0id]
Validating:  72% 18/25 [00:00<00:00, 50.70it/s][AEpoch 0:  97% 121/125 [00:05<00:00, 21.02it/s, loss=0.857, v_num=p0id]
Validating:  96% 24/25 [00:00<00:00, 52.93it/s][AEpoch 0: 100% 125/125 [00:05<00:00, 21.38it/s, loss=0.857, v_num=p0id]
                                               [AEpoch 0:   0% 0/125 [00:00<?, ?it/s, loss=0.857, v_num=p0id]          Epoch 1:   0% 0/125 [00:00<?, ?it/s, loss=0.857, v_num=p0id]Epoch 1:   1% 1/125 [00:00<00:06, 18.25it/s, loss=0.855, v_num=p0id]Epoch 1:   2% 2/125 [00:00<00:05, 20.78it/s, loss=0.855, v_num=p0id]Epoch 1:   2% 3/125 [00:00<00:06, 19.80it/s, loss=0.853, v_num=p0id]Epoch 1:   3% 4/125 [00:00<00:05, 21.41it/s, loss=0.853, v_num=p0id]Epoch 1:   4% 5/125 [00:00<00:05, 20.93it/s, loss=0.852, v_num=p0id]Epoch 1:   5% 6/125 [00:00<00:05, 20.98it/s, loss=0.852, v_num=p0id]Epoch 1:   6% 7/125 [00:00<00:05, 20.45it/s, loss=0.852, v_num=p0id]Epoch 1:   6% 7/125 [00:00<00:05, 20.44it/s, loss=0.847, v_num=p0id]Epoch 1:   6% 8/125 [00:00<00:05, 20.78it/s, loss=0.85, v_num=p0id] Epoch 1:   7% 9/125 [00:00<00:05, 20.84it/s, loss=0.848, v_num=p0id]Epoch 1:   8% 10/125 [00:00<00:05, 20.74it/s, loss=0.847, v_num=p0id]Epoch 1:   9% 11/125 [00:00<00:05, 20.67it/s, loss=0.847, v_num=p0id]Epoch 1:  10% 12/125 [00:00<00:05, 20.61it/s, loss=0.849, v_num=p0id]Epoch 1:  10% 13/125 [00:00<00:05, 20.69it/s, loss=0.849, v_num=p0id]Epoch 1:  11% 14/125 [00:00<00:05, 20.48it/s, loss=0.849, v_num=p0id]Epoch 1:  11% 14/125 [00:00<00:05, 20.47it/s, loss=0.847, v_num=p0id]Epoch 1:  12% 15/125 [00:00<00:05, 20.18it/s, loss=0.844, v_num=p0id]Epoch 1:  13% 16/125 [00:00<00:05, 20.04it/s, loss=0.843, v_num=p0id]Epoch 1:  14% 17/125 [00:00<00:05, 19.84it/s, loss=0.844, v_num=p0id]Epoch 1:  14% 18/125 [00:00<00:05, 19.90it/s, loss=0.839, v_num=p0id]Epoch 1:  15% 19/125 [00:00<00:05, 20.02it/s, loss=0.838, v_num=p0id]Epoch 1:  16% 20/125 [00:00<00:05, 20.02it/s, loss=0.838, v_num=p0id]Epoch 1:  17% 21/125 [00:01<00:05, 20.08it/s, loss=0.838, v_num=p0id]Epoch 1:  17% 21/125 [00:01<00:05, 20.08it/s, loss=0.837, v_num=p0id]Epoch 1:  18% 22/125 [00:01<00:05, 19.64it/s, loss=0.836, v_num=p0id]Epoch 1:  18% 23/125 [00:01<00:05, 19.79it/s, loss=0.838, v_num=p0id]Epoch 1:  19% 24/125 [00:01<00:05, 19.84it/s, loss=0.838, v_num=p0id]Epoch 1:  20% 25/125 [00:01<00:05, 19.96it/s, loss=0.837, v_num=p0id]Epoch 1:  21% 26/125 [00:01<00:04, 20.04it/s, loss=0.838, v_num=p0id]Epoch 1:  22% 27/125 [00:01<00:04, 20.13it/s, loss=0.841, v_num=p0id]Epoch 1:  22% 28/125 [00:01<00:04, 20.08it/s, loss=0.841, v_num=p0id]Epoch 1:  22% 28/125 [00:01<00:04, 20.08it/s, loss=0.841, v_num=p0id]Epoch 1:  23% 29/125 [00:01<00:04, 19.39it/s, loss=0.843, v_num=p0id]Epoch 1:  24% 30/125 [00:01<00:04, 19.46it/s, loss=0.844, v_num=p0id]Epoch 1:  25% 31/125 [00:01<00:04, 19.61it/s, loss=0.846, v_num=p0id]Epoch 1:  26% 32/125 [00:01<00:04, 19.65it/s, loss=0.846, v_num=p0id]Epoch 1:  26% 33/125 [00:01<00:04, 19.70it/s, loss=0.846, v_num=p0id]Epoch 1:  27% 34/125 [00:01<00:04, 19.72it/s, loss=0.846, v_num=p0id]Epoch 1:  28% 35/125 [00:01<00:04, 19.76it/s, loss=0.846, v_num=p0id]Epoch 1:  28% 35/125 [00:01<00:04, 19.76it/s, loss=0.847, v_num=p0id]Epoch 1:  29% 36/125 [00:01<00:04, 19.74it/s, loss=0.848, v_num=p0id]Epoch 1:  30% 37/125 [00:01<00:04, 19.14it/s, loss=0.85, v_num=p0id] Epoch 1:  30% 38/125 [00:01<00:04, 19.16it/s, loss=0.853, v_num=p0id]Epoch 1:  31% 39/125 [00:02<00:04, 19.22it/s, loss=0.855, v_num=p0id]Epoch 1:  32% 40/125 [00:02<00:04, 19.37it/s, loss=0.858, v_num=p0id]Epoch 1:  33% 41/125 [00:02<00:04, 19.45it/s, loss=0.858, v_num=p0id]Epoch 1:  34% 42/125 [00:02<00:04, 18.73it/s, loss=0.858, v_num=p0id]Epoch 1:  34% 42/125 [00:02<00:04, 18.73it/s, loss=0.858, v_num=p0id]Epoch 1:  34% 43/125 [00:02<00:04, 18.61it/s, loss=0.857, v_num=p0id]Epoch 1:  35% 44/125 [00:02<00:04, 18.67it/s, loss=0.859, v_num=p0id]Epoch 1:  36% 45/125 [00:02<00:04, 18.62it/s, loss=0.858, v_num=p0id]Epoch 1:  37% 46/125 [00:02<00:04, 18.47it/s, loss=0.857, v_num=p0id]Epoch 1:  38% 47/125 [00:02<00:04, 18.57it/s, loss=0.856, v_num=p0id]Epoch 1:  38% 48/125 [00:02<00:04, 18.58it/s, loss=0.853, v_num=p0id]Epoch 1:  39% 49/125 [00:02<00:04, 18.61it/s, loss=0.853, v_num=p0id]Epoch 1:  39% 49/125 [00:02<00:04, 18.61it/s, loss=0.85, v_num=p0id] Epoch 1:  40% 50/125 [00:02<00:04, 18.65it/s, loss=0.848, v_num=p0id]Epoch 1:  41% 51/125 [00:02<00:03, 18.70it/s, loss=0.845, v_num=p0id]Epoch 1:  42% 52/125 [00:02<00:03, 18.72it/s, loss=0.844, v_num=p0id]Epoch 1:  42% 53/125 [00:02<00:03, 18.68it/s, loss=0.843, v_num=p0id]Epoch 1:  43% 54/125 [00:02<00:03, 18.77it/s, loss=0.844, v_num=p0id]Epoch 1:  44% 55/125 [00:02<00:03, 18.77it/s, loss=0.845, v_num=p0id]Epoch 1:  45% 56/125 [00:03<00:03, 18.65it/s, loss=0.845, v_num=p0id]Epoch 1:  45% 56/125 [00:03<00:03, 18.65it/s, loss=0.845, v_num=p0id]Epoch 1:  46% 57/125 [00:03<00:03, 18.67it/s, loss=0.845, v_num=p0id]Epoch 1:  46% 58/125 [00:03<00:03, 18.74it/s, loss=0.845, v_num=p0id]Epoch 1:  47% 59/125 [00:03<00:03, 18.76it/s, loss=0.844, v_num=p0id]Epoch 1:  48% 60/125 [00:03<00:03, 18.78it/s, loss=0.841, v_num=p0id]Epoch 1:  49% 61/125 [00:03<00:03, 18.82it/s, loss=0.842, v_num=p0id]Epoch 1:  50% 62/125 [00:03<00:03, 18.87it/s, loss=0.842, v_num=p0id]Epoch 1:  50% 63/125 [00:03<00:03, 18.94it/s, loss=0.842, v_num=p0id]Epoch 1:  50% 63/125 [00:03<00:03, 18.94it/s, loss=0.841, v_num=p0id]Epoch 1:  51% 64/125 [00:03<00:03, 18.76it/s, loss=0.84, v_num=p0id] Epoch 1:  52% 65/125 [00:03<00:03, 18.80it/s, loss=0.84, v_num=p0id]Epoch 1:  53% 66/125 [00:03<00:03, 18.84it/s, loss=0.839, v_num=p0id]Epoch 1:  54% 67/125 [00:03<00:03, 18.84it/s, loss=0.838, v_num=p0id]Epoch 1:  54% 68/125 [00:03<00:03, 18.86it/s, loss=0.84, v_num=p0id] Epoch 1:  55% 69/125 [00:03<00:02, 18.90it/s, loss=0.843, v_num=p0id]Epoch 1:  56% 70/125 [00:03<00:02, 18.89it/s, loss=0.843, v_num=p0id]Epoch 1:  56% 70/125 [00:03<00:02, 18.88it/s, loss=0.843, v_num=p0id]Epoch 1:  57% 71/125 [00:03<00:02, 18.90it/s, loss=0.844, v_num=p0id]Epoch 1:  58% 72/125 [00:03<00:02, 18.88it/s, loss=0.843, v_num=p0id]Epoch 1:  58% 73/125 [00:04<00:02, 17.62it/s, loss=0.843, v_num=p0id]Epoch 1:  59% 74/125 [00:04<00:02, 17.52it/s, loss=0.843, v_num=p0id]Epoch 1:  60% 75/125 [00:04<00:02, 17.56it/s, loss=0.845, v_num=p0id]Epoch 1:  61% 76/125 [00:04<00:02, 17.53it/s, loss=0.844, v_num=p0id]Epoch 1:  62% 77/125 [00:04<00:02, 17.50it/s, loss=0.844, v_num=p0id]Epoch 1:  62% 77/125 [00:04<00:02, 17.50it/s, loss=0.841, v_num=p0id]Epoch 1:  62% 78/125 [00:04<00:02, 17.52it/s, loss=0.842, v_num=p0id]Epoch 1:  63% 79/125 [00:04<00:02, 17.55it/s, loss=0.842, v_num=p0id]Epoch 1:  64% 80/125 [00:04<00:02, 17.54it/s, loss=0.841, v_num=p0id]Epoch 1:  65% 81/125 [00:04<00:02, 17.60it/s, loss=0.84, v_num=p0id] Epoch 1:  66% 82/125 [00:04<00:02, 17.65it/s, loss=0.842, v_num=p0id]Epoch 1:  66% 83/125 [00:04<00:02, 17.65it/s, loss=0.841, v_num=p0id]Epoch 1:  67% 84/125 [00:04<00:02, 17.66it/s, loss=0.841, v_num=p0id]Epoch 1:  67% 84/125 [00:04<00:02, 17.66it/s, loss=0.84, v_num=p0id] Epoch 1:  68% 85/125 [00:04<00:02, 17.70it/s, loss=0.84, v_num=p0id]Epoch 1:  69% 86/125 [00:04<00:02, 17.71it/s, loss=0.841, v_num=p0id]Epoch 1:  70% 87/125 [00:04<00:02, 17.75it/s, loss=0.842, v_num=p0id]Epoch 1:  70% 88/125 [00:04<00:02, 17.76it/s, loss=0.842, v_num=p0id]Epoch 1:  71% 89/125 [00:05<00:02, 17.79it/s, loss=0.84, v_num=p0id] Epoch 1:  72% 90/125 [00:05<00:01, 17.83it/s, loss=0.841, v_num=p0id]Epoch 1:  73% 91/125 [00:05<00:01, 17.83it/s, loss=0.841, v_num=p0id]Epoch 1:  73% 91/125 [00:05<00:01, 17.83it/s, loss=0.841, v_num=p0id]Epoch 1:  74% 92/125 [00:05<00:01, 17.80it/s, loss=0.84, v_num=p0id] Epoch 1:  74% 93/125 [00:05<00:01, 17.81it/s, loss=0.84, v_num=p0id]Epoch 1:  75% 94/125 [00:05<00:01, 17.86it/s, loss=0.84, v_num=p0id]Epoch 1:  76% 95/125 [00:05<00:01, 17.90it/s, loss=0.84, v_num=p0id]Epoch 1:  77% 96/125 [00:05<00:01, 17.94it/s, loss=0.84, v_num=p0id]Epoch 1:  78% 97/125 [00:05<00:01, 17.96it/s, loss=0.84, v_num=p0id]Epoch 1:  78% 98/125 [00:05<00:01, 18.01it/s, loss=0.84, v_num=p0id]Epoch 1:  78% 98/125 [00:05<00:01, 18.01it/s, loss=0.84, v_num=p0id]Epoch 1:  79% 99/125 [00:05<00:01, 18.00it/s, loss=0.84, v_num=p0id]Epoch 1:  80% 100/125 [00:05<00:01, 17.92it/s, loss=0.841, v_num=p0id]
Validating: 0it [00:00, ?it/s]/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have r[A
Validating:   0% 0/25 [00:00<?, ?it/s][Aequires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Epoch 1:  84% 105/125 [00:05<00:01, 18.55it/s, loss=0.841, v_num=p0id]
Validating:  24% 6/25 [00:00<00:00, 53.61it/s][AEpoch 1:  90% 112/125 [00:05<00:00, 19.32it/s, loss=0.841, v_num=p0id]
Validating:  48% 12/25 [00:00<00:00, 51.24it/s][A
Validating:  72% 18/25 [00:00<00:00, 53.07it/s][AEpoch 1:  95% 119/125 [00:05<00:00, 20.09it/s, loss=0.841, v_num=p0id]
Validating:  96% 24/25 [00:00<00:00, 51.08it/s][AEpoch 1: 100% 125/125 [00:06<00:00, 20.60it/s, loss=0.841, v_num=p0id]
                                               [AEpoch 1:   0% 0/125 [00:00<?, ?it/s, loss=0.841, v_num=p0id]          Epoch 2:   0% 0/125 [00:00<?, ?it/s, loss=0.841, v_num=p0id]Epoch 2:   1% 1/125 [00:00<00:07, 17.36it/s, loss=0.841, v_num=p0id]Epoch 2:   2% 2/125 [00:00<00:06, 20.42it/s, loss=0.841, v_num=p0id]Epoch 2:   2% 3/125 [00:00<00:06, 19.44it/s, loss=0.841, v_num=p0id]Epoch 2:   3% 4/125 [00:00<00:06, 19.30it/s, loss=0.842, v_num=p0id]Epoch 2:   4% 5/125 [00:00<00:06, 18.66it/s, loss=0.842, v_num=p0id]Epoch 2:   5% 6/125 [00:00<00:06, 19.06it/s, loss=0.842, v_num=p0id]Epoch 2:   6% 7/125 [00:00<00:06, 18.85it/s, loss=0.842, v_num=p0id]Epoch 2:   6% 7/125 [00:00<00:06, 18.84it/s, loss=0.839, v_num=p0id]Epoch 2:   6% 8/125 [00:00<00:06, 19.31it/s, loss=0.842, v_num=p0id]Epoch 2:   7% 9/125 [00:00<00:05, 19.53it/s, loss=0.841, v_num=p0id]Epoch 2:   8% 10/125 [00:00<00:05, 19.58it/s, loss=0.841, v_num=p0id]Epoch 2:   9% 11/125 [00:00<00:05, 19.62it/s, loss=0.841, v_num=p0id]Epoch 2:  10% 12/125 [00:00<00:05, 19.66it/s, loss=0.845, v_num=p0id]Epoch 2:  10% 13/125 [00:00<00:05, 19.80it/s, loss=0.845, v_num=p0id]Epoch 2:  11% 14/125 [00:00<00:05, 19.08it/s, loss=0.845, v_num=p0id]Epoch 2:  11% 14/125 [00:00<00:05, 19.07it/s, loss=0.844, v_num=p0id]Epoch 2:  12% 15/125 [00:00<00:05, 18.83it/s, loss=0.842, v_num=p0id]Epoch 2:  13% 16/125 [00:00<00:06, 17.69it/s, loss=0.842, v_num=p0id]Epoch 2:  14% 17/125 [00:00<00:06, 17.83it/s, loss=0.843, v_num=p0id]Epoch 2:  14% 18/125 [00:01<00:06, 17.54it/s, loss=0.839, v_num=p0id]Epoch 2:  15% 19/125 [00:01<00:05, 17.74it/s, loss=0.838, v_num=p0id]Epoch 2:  16% 20/125 [00:01<00:05, 17.84it/s, loss=0.839, v_num=p0id]Epoch 2:  17% 21/125 [00:01<00:05, 17.99it/s, loss=0.839, v_num=p0id]Epoch 2:  17% 21/125 [00:01<00:05, 17.98it/s, loss=0.839, v_num=p0id]Epoch 2:  18% 22/125 [00:01<00:05, 18.04it/s, loss=0.839, v_num=p0id]Epoch 2:  18% 23/125 [00:01<00:05, 18.23it/s, loss=0.842, v_num=p0id]Epoch 2:  19% 24/125 [00:01<00:05, 18.34it/s, loss=0.843, v_num=p0id]Epoch 2:  20% 25/125 [00:01<00:05, 18.50it/s, loss=0.844, v_num=p0id]Epoch 2:  21% 26/125 [00:01<00:05, 18.50it/s, loss=0.844, v_num=p0id]Epoch 2:  22% 27/125 [00:01<00:05, 18.63it/s, loss=0.848, v_num=p0id]Epoch 2:  22% 28/125 [00:01<00:05, 18.65it/s, loss=0.848, v_num=p0id]Epoch 2:  22% 28/125 [00:01<00:05, 18.65it/s, loss=0.847, v_num=p0id]Epoch 2:  23% 29/125 [00:01<00:05, 18.77it/s, loss=0.85, v_num=p0id] Epoch 2:  24% 30/125 [00:01<00:05, 18.86it/s, loss=0.851, v_num=p0id]Epoch 2:  25% 31/125 [00:01<00:04, 19.03it/s, loss=0.853, v_num=p0id]Epoch 2:  26% 32/125 [00:01<00:04, 19.09it/s, loss=0.853, v_num=p0id]Epoch 2:  26% 33/125 [00:01<00:04, 19.08it/s, loss=0.855, v_num=p0id]Epoch 2:  27% 34/125 [00:01<00:04, 18.98it/s, loss=0.855, v_num=p0id]Epoch 2:  28% 35/125 [00:01<00:04, 18.77it/s, loss=0.855, v_num=p0id]Epoch 2:  28% 35/125 [00:01<00:04, 18.77it/s, loss=0.856, v_num=p0id]Epoch 2:  29% 36/125 [00:01<00:04, 18.90it/s, loss=0.857, v_num=p0id]Epoch 2:  30% 37/125 [00:01<00:04, 19.08it/s, loss=0.859, v_num=p0id]Epoch 2:  30% 38/125 [00:01<00:04, 19.12it/s, loss=0.863, v_num=p0id]Epoch 2:  31% 39/125 [00:02<00:04, 19.19it/s, loss=0.867, v_num=p0id]Epoch 2:  32% 40/125 [00:02<00:04, 19.33it/s, loss=0.869, v_num=p0id]Epoch 2:  33% 41/125 [00:02<00:04, 19.43it/s, loss=0.871, v_num=p0id]Epoch 2:  34% 42/125 [00:02<00:04, 19.48it/s, loss=0.871, v_num=p0id]Epoch 2:  34% 42/125 [00:02<00:04, 19.47it/s, loss=0.872, v_num=p0id]Epoch 2:  34% 43/125 [00:02<00:04, 19.46it/s, loss=0.871, v_num=p0id]Epoch 2:  35% 44/125 [00:02<00:04, 19.49it/s, loss=0.872, v_num=p0id]Epoch 2:  36% 45/125 [00:02<00:04, 19.52it/s, loss=0.872, v_num=p0id]Epoch 2:  37% 46/125 [00:02<00:04, 19.56it/s, loss=0.872, v_num=p0id]Epoch 2:  38% 47/125 [00:02<00:03, 19.65it/s, loss=0.871, v_num=p0id]Epoch 2:  38% 48/125 [00:02<00:03, 19.55it/s, loss=0.87, v_num=p0id] Epoch 2:  39% 49/125 [00:02<00:03, 19.47it/s, loss=0.87, v_num=p0id]Epoch 2:  39% 49/125 [00:02<00:03, 19.47it/s, loss=0.866, v_num=p0id]Epoch 2:  40% 50/125 [00:02<00:03, 19.17it/s, loss=0.864, v_num=p0id]Epoch 2:  41% 51/125 [00:02<00:03, 18.83it/s, loss=0.86, v_num=p0id] Epoch 2:  42% 52/125 [00:02<00:03, 18.91it/s, loss=0.859, v_num=p0id]Epoch 2:  42% 53/125 [00:02<00:03, 18.96it/s, loss=0.858, v_num=p0id]Epoch 2:  43% 54/125 [00:02<00:03, 19.04it/s, loss=0.858, v_num=p0id]Epoch 2:  44% 55/125 [00:02<00:03, 19.06it/s, loss=0.858, v_num=p0id]Epoch 2:  45% 56/125 [00:02<00:03, 19.09it/s, loss=0.858, v_num=p0id]Epoch 2:  45% 56/125 [00:02<00:03, 19.09it/s, loss=0.858, v_num=p0id]Epoch 2:  46% 57/125 [00:03<00:03, 18.89it/s, loss=0.857, v_num=p0id]Epoch 2:  46% 58/125 [00:03<00:03, 18.79it/s, loss=0.856, v_num=p0id]Epoch 2:  47% 59/125 [00:03<00:03, 18.81it/s, loss=0.854, v_num=p0id]Epoch 2:  48% 60/125 [00:03<00:03, 18.83it/s, loss=0.851, v_num=p0id]Epoch 2:  49% 61/125 [00:03<00:03, 18.87it/s, loss=0.848, v_num=p0id]Epoch 2:  50% 62/125 [00:03<00:03, 18.92it/s, loss=0.847, v_num=p0id]Epoch 2:  50% 63/125 [00:03<00:03, 18.98it/s, loss=0.847, v_num=p0id]Epoch 2:  50% 63/125 [00:03<00:03, 18.98it/s, loss=0.845, v_num=p0id]Epoch 2:  51% 64/125 [00:03<00:03, 19.03it/s, loss=0.842, v_num=p0id]Epoch 2:  52% 65/125 [00:03<00:03, 19.06it/s, loss=0.841, v_num=p0id]Epoch 2:  53% 66/125 [00:03<00:03, 19.09it/s, loss=0.84, v_num=p0id] Epoch 2:  54% 67/125 [00:03<00:03, 19.10it/s, loss=0.838, v_num=p0id]Epoch 2:  54% 68/125 [00:03<00:02, 19.13it/s, loss=0.839, v_num=p0id]Epoch 2:  55% 69/125 [00:03<00:02, 19.16it/s, loss=0.84, v_num=p0id] Epoch 2:  56% 70/125 [00:03<00:02, 19.15it/s, loss=0.84, v_num=p0id]Epoch 2:  56% 70/125 [00:03<00:02, 19.14it/s, loss=0.838, v_num=p0id]Epoch 2:  57% 71/125 [00:03<00:02, 19.09it/s, loss=0.838, v_num=p0id]Epoch 2:  58% 72/125 [00:03<00:02, 19.07it/s, loss=0.836, v_num=p0id]Epoch 2:  58% 73/125 [00:03<00:02, 19.08it/s, loss=0.835, v_num=p0id]Epoch 2:  59% 74/125 [00:03<00:02, 19.06it/s, loss=0.833, v_num=p0id]Epoch 2:  60% 75/125 [00:03<00:02, 19.09it/s, loss=0.834, v_num=p0id]Epoch 2:  61% 76/125 [00:03<00:02, 19.07it/s, loss=0.832, v_num=p0id]Epoch 2:  62% 77/125 [00:04<00:02, 19.05it/s, loss=0.832, v_num=p0id]Epoch 2:  62% 77/125 [00:04<00:02, 19.05it/s, loss=0.83, v_num=p0id] Epoch 2:  62% 78/125 [00:04<00:02, 18.81it/s, loss=0.83, v_num=p0id]Epoch 2:  63% 79/125 [00:04<00:02, 18.82it/s, loss=0.829, v_num=p0id]Epoch 2:  64% 80/125 [00:04<00:02, 18.76it/s, loss=0.828, v_num=p0id]Epoch 2:  65% 81/125 [00:04<00:02, 18.69it/s, loss=0.827, v_num=p0id]Epoch 2:  66% 82/125 [00:04<00:02, 18.73it/s, loss=0.828, v_num=p0id]Epoch 2:  66% 83/125 [00:04<00:02, 18.71it/s, loss=0.827, v_num=p0id]Epoch 2:  67% 84/125 [00:04<00:02, 18.71it/s, loss=0.827, v_num=p0id]Epoch 2:  67% 84/125 [00:04<00:02, 18.71it/s, loss=0.826, v_num=p0id]Epoch 2:  68% 85/125 [00:04<00:02, 18.74it/s, loss=0.826, v_num=p0id]Epoch 2:  69% 86/125 [00:04<00:02, 18.74it/s, loss=0.827, v_num=p0id]Epoch 2:  70% 87/125 [00:04<00:02, 18.76it/s, loss=0.829, v_num=p0id]Epoch 2:  70% 88/125 [00:04<00:01, 18.76it/s, loss=0.83, v_num=p0id] Epoch 2:  71% 89/125 [00:04<00:01, 18.75it/s, loss=0.835, v_num=p0id]Epoch 2:  72% 90/125 [00:04<00:01, 18.28it/s, loss=0.84, v_num=p0id] Epoch 2:  73% 91/125 [00:04<00:01, 18.27it/s, loss=0.84, v_num=p0id]Epoch 2:  73% 91/125 [00:04<00:01, 18.27it/s, loss=0.844, v_num=p0id]Epoch 2:  74% 92/125 [00:05<00:01, 17.83it/s, loss=0.848, v_num=p0id]Epoch 2:  74% 93/125 [00:05<00:01, 17.85it/s, loss=0.85, v_num=p0id] Epoch 2:  75% 94/125 [00:05<00:01, 17.89it/s, loss=0.854, v_num=p0id]Epoch 2:  76% 95/125 [00:05<00:01, 17.93it/s, loss=0.855, v_num=p0id]Epoch 2:  77% 96/125 [00:05<00:01, 17.97it/s, loss=0.858, v_num=p0id]Epoch 2:  78% 97/125 [00:05<00:01, 17.98it/s, loss=0.859, v_num=p0id]Epoch 2:  78% 98/125 [00:05<00:01, 18.03it/s, loss=0.859, v_num=p0id]Epoch 2:  78% 98/125 [00:05<00:01, 18.03it/s, loss=0.864, v_num=p0id]Epoch 2:  79% 99/125 [00:05<00:01, 18.01it/s, loss=0.865, v_num=p0id]Epoch 2:  80% 100/125 [00:05<00:01, 17.95it/s, loss=0.867, v_num=p0id]
Validating: 0it [00:00, ?it/s][A
Validating:   0% 0/25 [00:00<?, ?it/s][A/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")

Validating:  16% 4/25 [00:00<00:00, 39.03it/s][AEpoch 2:  84% 105/125 [00:05<00:01, 18.49it/s, loss=0.867, v_num=p0id]
Validating:  40% 10/25 [00:00<00:00, 47.24it/s][AEpoch 2:  90% 112/125 [00:05<00:00, 19.25it/s, loss=0.867, v_num=p0id]
Validating:  64% 16/25 [00:00<00:00, 50.16it/s][AEpoch 2:  95% 119/125 [00:05<00:00, 19.96it/s, loss=0.867, v_num=p0id]
Validating:  84% 21/25 [00:00<00:00, 48.81it/s][AEpoch 2: 100% 125/125 [00:06<00:00, 20.54it/s, loss=0.867, v_num=p0id]
                                               [AEpoch 2:   0% 0/125 [00:00<?, ?it/s, loss=0.867, v_num=p0id]          Epoch 3:   0% 0/125 [00:00<?, ?it/s, loss=0.867, v_num=p0id]Epoch 3:   1% 1/125 [00:00<00:06, 17.76it/s, loss=0.868, v_num=p0id]Epoch 3:   2% 2/125 [00:00<00:07, 16.23it/s, loss=0.87, v_num=p0id] Epoch 3:   2% 3/125 [00:00<00:07, 16.82it/s, loss=0.87, v_num=p0id]Epoch 3:   3% 4/125 [00:00<00:06, 18.74it/s, loss=0.871, v_num=p0id]Epoch 3:   4% 5/125 [00:00<00:06, 19.15it/s, loss=0.872, v_num=p0id]Epoch 3:   5% 6/125 [00:00<00:06, 19.48it/s, loss=0.874, v_num=p0id]Epoch 3:   6% 7/125 [00:00<00:06, 19.23it/s, loss=0.874, v_num=p0id]Epoch 3:   6% 7/125 [00:00<00:06, 19.22it/s, loss=0.871, v_num=p0id]Epoch 3:   6% 8/125 [00:00<00:05, 19.69it/s, loss=0.871, v_num=p0id]Epoch 3:   7% 9/125 [00:00<00:05, 19.89it/s, loss=0.867, v_num=p0id]Epoch 3:   8% 10/125 [00:00<00:05, 19.95it/s, loss=0.864, v_num=p0id]Epoch 3:   9% 11/125 [00:00<00:05, 19.84it/s, loss=0.862, v_num=p0id]Epoch 3:  10% 12/125 [00:00<00:05, 19.77it/s, loss=0.86, v_num=p0id] Epoch 3:  10% 13/125 [00:00<00:05, 19.91it/s, loss=0.859, v_num=p0id]Epoch 3:  11% 14/125 [00:00<00:05, 19.77it/s, loss=0.859, v_num=p0id]Epoch 3:  11% 14/125 [00:00<00:05, 19.76it/s, loss=0.855, v_num=p0id]Epoch 3:  12% 15/125 [00:00<00:05, 19.88it/s, loss=0.851, v_num=p0id]Epoch 3:  13% 16/125 [00:00<00:05, 19.99it/s, loss=0.849, v_num=p0id]Epoch 3:  14% 17/125 [00:00<00:05, 20.12it/s, loss=0.849, v_num=p0id]Epoch 3:  14% 18/125 [00:00<00:05, 20.11it/s, loss=0.841, v_num=p0id]Epoch 3:  15% 19/125 [00:00<00:05, 20.23it/s, loss=0.839, v_num=p0id]Epoch 3:  16% 20/125 [00:00<00:05, 20.07it/s, loss=0.838, v_num=p0id]Epoch 3:  17% 21/125 [00:01<00:05, 19.89it/s, loss=0.838, v_num=p0id]Epoch 3:  17% 21/125 [00:01<00:05, 19.88it/s, loss=0.836, v_num=p0id]Epoch 3:  18% 22/125 [00:01<00:05, 19.87it/s, loss=0.835, v_num=p0id]Epoch 3:  18% 23/125 [00:01<00:05, 19.86it/s, loss=0.836, v_num=p0id]Epoch 3:  19% 24/125 [00:01<00:05, 19.93it/s, loss=0.836, v_num=p0id]Epoch 3:  20% 25/125 [00:01<00:04, 20.06it/s, loss=0.835, v_num=p0id]Epoch 3:  21% 26/125 [00:01<00:04, 20.03it/s, loss=0.834, v_num=p0id]Epoch 3:  22% 27/125 [00:01<00:04, 20.13it/s, loss=0.836, v_num=p0id]Epoch 3:  22% 28/125 [00:01<00:04, 20.10it/s, loss=0.836, v_num=p0id]Epoch 3:  22% 28/125 [00:01<00:04, 20.10it/s, loss=0.835, v_num=p0id]Epoch 3:  23% 29/125 [00:01<00:04, 20.19it/s, loss=0.836, v_num=p0id]Epoch 3:  24% 30/125 [00:01<00:04, 20.24it/s, loss=0.837, v_num=p0id]Epoch 3:  25% 31/125 [00:01<00:04, 20.39it/s, loss=0.838, v_num=p0id]Epoch 3:  26% 32/125 [00:01<00:04, 20.42it/s, loss=0.838, v_num=p0id]Epoch 3:  26% 33/125 [00:01<00:04, 20.45it/s, loss=0.838, v_num=p0id]Epoch 3:  27% 34/125 [00:01<00:04, 20.45it/s, loss=0.839, v_num=p0id]Epoch 3:  28% 35/125 [00:01<00:04, 20.48it/s, loss=0.839, v_num=p0id]Epoch 3:  28% 35/125 [00:01<00:04, 20.48it/s, loss=0.839, v_num=p0id]Epoch 3:  29% 36/125 [00:01<00:04, 20.58it/s, loss=0.84, v_num=p0id] Epoch 3:  30% 37/125 [00:01<00:04, 20.77it/s, loss=0.842, v_num=p0id]Epoch 3:  30% 38/125 [00:01<00:04, 20.77it/s, loss=0.845, v_num=p0id]Epoch 3:  31% 39/125 [00:01<00:04, 20.80it/s, loss=0.847, v_num=p0id]Epoch 3:  32% 40/125 [00:01<00:04, 20.92it/s, loss=0.849, v_num=p0id]Epoch 3:  33% 41/125 [00:01<00:04, 20.99it/s, loss=0.849, v_num=p0id]Epoch 3:  34% 42/125 [00:01<00:03, 21.04it/s, loss=0.849, v_num=p0id]Epoch 3:  34% 42/125 [00:01<00:03, 21.03it/s, loss=0.848, v_num=p0id]Epoch 3:  34% 43/125 [00:02<00:03, 21.02it/s, loss=0.848, v_num=p0id]Epoch 3:  35% 44/125 [00:02<00:03, 21.01it/s, loss=0.848, v_num=p0id]Epoch 3:  36% 45/125 [00:02<00:03, 21.01it/s, loss=0.849, v_num=p0id]Epoch 3:  37% 46/125 [00:02<00:03, 21.02it/s, loss=0.848, v_num=p0id]Epoch 3:  38% 47/125 [00:02<00:03, 21.08it/s, loss=0.848, v_num=p0id]Epoch 3:  38% 48/125 [00:02<00:03, 21.04it/s, loss=0.846, v_num=p0id]Epoch 3:  39% 49/125 [00:02<00:03, 21.02it/s, loss=0.846, v_num=p0id]Epoch 3:  39% 49/125 [00:02<00:03, 21.01it/s, loss=0.843, v_num=p0id]Epoch 3:  40% 50/125 [00:02<00:03, 21.02it/s, loss=0.842, v_num=p0id]Epoch 3:  41% 51/125 [00:02<00:03, 21.04it/s, loss=0.84, v_num=p0id] Epoch 3:  42% 52/125 [00:02<00:03, 21.10it/s, loss=0.839, v_num=p0id]Epoch 3:  42% 53/125 [00:02<00:03, 21.09it/s, loss=0.839, v_num=p0id]Epoch 3:  43% 54/125 [00:02<00:03, 21.03it/s, loss=0.84, v_num=p0id] Epoch 3:  44% 55/125 [00:02<00:03, 20.98it/s, loss=0.842, v_num=p0id]Epoch 3:  45% 56/125 [00:02<00:03, 21.00it/s, loss=0.842, v_num=p0id]Epoch 3:  45% 56/125 [00:02<00:03, 21.00it/s, loss=0.842, v_num=p0id]Epoch 3:  46% 57/125 [00:02<00:03, 21.09it/s, loss=0.843, v_num=p0id]Epoch 3:  46% 58/125 [00:02<00:03, 21.13it/s, loss=0.844, v_num=p0id]Epoch 3:  47% 59/125 [00:02<00:03, 21.11it/s, loss=0.843, v_num=p0id]Epoch 3:  48% 60/125 [00:02<00:03, 21.10it/s, loss=0.841, v_num=p0id]Epoch 3:  49% 61/125 [00:02<00:03, 21.11it/s, loss=0.842, v_num=p0id]Epoch 3:  50% 62/125 [00:02<00:02, 21.13it/s, loss=0.842, v_num=p0id]Epoch 3:  50% 63/125 [00:02<00:02, 21.18it/s, loss=0.842, v_num=p0id]Epoch 3:  50% 63/125 [00:02<00:02, 21.17it/s, loss=0.842, v_num=p0id]Epoch 3:  51% 64/125 [00:03<00:02, 21.19it/s, loss=0.841, v_num=p0id]Epoch 3:  52% 65/125 [00:03<00:02, 21.14it/s, loss=0.841, v_num=p0id]Epoch 3:  53% 66/125 [00:03<00:02, 21.16it/s, loss=0.84, v_num=p0id] Epoch 3:  54% 67/125 [00:03<00:02, 21.14it/s, loss=0.839, v_num=p0id]Epoch 3:  54% 68/125 [00:03<00:02, 21.12it/s, loss=0.84, v_num=p0id] Epoch 3:  55% 69/125 [00:03<00:02, 20.25it/s, loss=0.842, v_num=p0id]Epoch 3:  56% 70/125 [00:03<00:02, 20.22it/s, loss=0.842, v_num=p0id]Epoch 3:  56% 70/125 [00:03<00:02, 20.22it/s, loss=0.84, v_num=p0id] Epoch 3:  57% 71/125 [00:03<00:02, 20.22it/s, loss=0.841, v_num=p0id]Epoch 3:  58% 72/125 [00:03<00:02, 20.18it/s, loss=0.839, v_num=p0id]Epoch 3:  58% 73/125 [00:03<00:02, 20.13it/s, loss=0.837, v_num=p0id]Epoch 3:  59% 74/125 [00:03<00:02, 18.89it/s, loss=0.836, v_num=p0id]Epoch 3:  60% 75/125 [00:04<00:02, 17.70it/s, loss=0.836, v_num=p0id]Epoch 3:  61% 76/125 [00:04<00:02, 16.98it/s, loss=0.834, v_num=p0id]Epoch 3:  62% 77/125 [00:04<00:02, 16.92it/s, loss=0.834, v_num=p0id]Epoch 3:  62% 77/125 [00:04<00:02, 16.92it/s, loss=0.831, v_num=p0id]Epoch 3:  62% 78/125 [00:04<00:02, 16.95it/s, loss=0.83, v_num=p0id] Epoch 3:  63% 79/125 [00:04<00:02, 16.98it/s, loss=0.83, v_num=p0id]Epoch 3:  64% 80/125 [00:04<00:02, 16.98it/s, loss=0.828, v_num=p0id]Epoch 3:  65% 81/125 [00:04<00:02, 17.05it/s, loss=0.827, v_num=p0id]Epoch 3:  66% 82/125 [00:04<00:02, 17.10it/s, loss=0.829, v_num=p0id]Epoch 3:  66% 83/125 [00:04<00:02, 17.11it/s, loss=0.827, v_num=p0id]Epoch 3:  67% 84/125 [00:04<00:02, 17.13it/s, loss=0.827, v_num=p0id]Epoch 3:  67% 84/125 [00:04<00:02, 17.13it/s, loss=0.826, v_num=p0id]Epoch 3:  68% 85/125 [00:04<00:02, 17.17it/s, loss=0.826, v_num=p0id]Epoch 3:  69% 86/125 [00:05<00:02, 17.19it/s, loss=0.827, v_num=p0id]Epoch 3:  70% 87/125 [00:05<00:02, 17.19it/s, loss=0.828, v_num=p0id]Epoch 3:  70% 88/125 [00:05<00:02, 16.70it/s, loss=0.827, v_num=p0id]Epoch 3:  71% 89/125 [00:05<00:02, 16.74it/s, loss=0.827, v_num=p0id]Epoch 3:  72% 90/125 [00:05<00:02, 16.79it/s, loss=0.828, v_num=p0id]Epoch 3:  73% 91/125 [00:05<00:02, 16.80it/s, loss=0.828, v_num=p0id]Epoch 3:  73% 91/125 [00:05<00:02, 16.80it/s, loss=0.828, v_num=p0id]Epoch 3:  74% 92/125 [00:05<00:01, 16.75it/s, loss=0.827, v_num=p0id]Epoch 3:  74% 93/125 [00:05<00:01, 16.77it/s, loss=0.827, v_num=p0id]Epoch 3:  75% 94/125 [00:05<00:01, 16.80it/s, loss=0.829, v_num=p0id]Epoch 3:  76% 95/125 [00:05<00:01, 16.85it/s, loss=0.831, v_num=p0id]Epoch 3:  77% 96/125 [00:05<00:01, 16.89it/s, loss=0.832, v_num=p0id]Epoch 3:  78% 97/125 [00:05<00:01, 16.91it/s, loss=0.834, v_num=p0id]Epoch 3:  78% 98/125 [00:05<00:01, 16.96it/s, loss=0.834, v_num=p0id]Epoch 3:  78% 98/125 [00:05<00:01, 16.96it/s, loss=0.834, v_num=p0id]Epoch 3:  79% 99/125 [00:05<00:01, 16.97it/s, loss=0.837, v_num=p0id]Epoch 3:  80% 100/125 [00:05<00:01, 16.92it/s, loss=0.839, v_num=p0id]
Validating: 0it [00:00, ?it/s][A/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")

Validating:   0% 0/25 [00:00<?, ?it/s][AEpoch 3:  84% 105/125 [00:05<00:01, 17.53it/s, loss=0.839, v_num=p0id]
Validating:  24% 6/25 [00:00<00:00, 52.68it/s][AEpoch 3:  90% 112/125 [00:06<00:00, 18.28it/s, loss=0.839, v_num=p0id]
Validating:  48% 12/25 [00:00<00:00, 50.91it/s][A
Validating:  72% 18/25 [00:00<00:00, 52.58it/s][AEpoch 3:  95% 119/125 [00:06<00:00, 19.02it/s, loss=0.839, v_num=p0id]
Validating:  96% 24/25 [00:00<00:00, 52.75it/s][AEpoch 3: 100% 125/125 [00:06<00:00, 19.50it/s, loss=0.839, v_num=p0id]
                                               [AEpoch 3:   0% 0/125 [00:00<?, ?it/s, loss=0.839, v_num=p0id]          Epoch 4:   0% 0/125 [00:00<?, ?it/s, loss=0.839, v_num=p0id]Epoch 4:   1% 1/125 [00:00<00:07, 17.19it/s, loss=0.84, v_num=p0id]Epoch 4:   2% 2/125 [00:00<00:08, 14.15it/s, loss=0.841, v_num=p0id]Epoch 4:   2% 3/125 [00:00<00:08, 15.22it/s, loss=0.841, v_num=p0id]Epoch 4:   3% 4/125 [00:00<00:07, 16.60it/s, loss=0.843, v_num=p0id]Epoch 4:   4% 5/125 [00:00<00:06, 17.33it/s, loss=0.845, v_num=p0id]Epoch 4:   5% 6/125 [00:00<00:08, 14.52it/s, loss=0.846, v_num=p0id]Epoch 4:   6% 7/125 [00:00<00:08, 14.08it/s, loss=0.846, v_num=p0id]Epoch 4:   6% 7/125 [00:00<00:08, 14.08it/s, loss=0.844, v_num=p0id]Epoch 4:   6% 8/125 [00:00<00:07, 14.83it/s, loss=0.848, v_num=p0id]Epoch 4:   7% 9/125 [00:00<00:07, 15.36it/s, loss=0.848, v_num=p0id]Epoch 4:   8% 10/125 [00:00<00:07, 15.76it/s, loss=0.849, v_num=p0id]Epoch 4:   9% 11/125 [00:00<00:07, 15.81it/s, loss=0.851, v_num=p0id]Epoch 4:  10% 12/125 [00:00<00:07, 15.03it/s, loss=0.855, v_num=p0id]Epoch 4:  10% 13/125 [00:00<00:07, 15.39it/s, loss=0.855, v_num=p0id]Epoch 4:  11% 14/125 [00:00<00:07, 15.32it/s, loss=0.855, v_num=p0id]Epoch 4:  11% 14/125 [00:00<00:07, 15.32it/s, loss=0.854, v_num=p0id]Epoch 4:  12% 15/125 [00:00<00:07, 15.61it/s, loss=0.85, v_num=p0id] Epoch 4:  13% 16/125 [00:01<00:06, 15.89it/s, loss=0.849, v_num=p0id]Epoch 4:  14% 17/125 [00:01<00:06, 16.17it/s, loss=0.849, v_num=p0id]Epoch 4:  14% 18/125 [00:01<00:06, 16.38it/s, loss=0.846, v_num=p0id]Epoch 4:  15% 19/125 [00:01<00:06, 16.45it/s, loss=0.843, v_num=p0id]Epoch 4:  16% 20/125 [00:01<00:06, 16.43it/s, loss=0.842, v_num=p0id]Epoch 4:  17% 21/125 [00:01<00:06, 16.62it/s, loss=0.842, v_num=p0id]Epoch 4:  17% 21/125 [00:01<00:06, 16.61it/s, loss=0.841, v_num=p0id]Epoch 4:  18% 22/125 [00:01<00:06, 16.73it/s, loss=0.84, v_num=p0id] Epoch 4:  18% 23/125 [00:01<00:06, 16.95it/s, loss=0.841, v_num=p0id]Epoch 4:  19% 24/125 [00:01<00:05, 17.10it/s, loss=0.84, v_num=p0id] Epoch 4:  20% 25/125 [00:01<00:05, 17.29it/s, loss=0.839, v_num=p0id]Epoch 4:  21% 26/125 [00:01<00:05, 17.45it/s, loss=0.838, v_num=p0id]Epoch 4:  22% 27/125 [00:01<00:05, 17.61it/s, loss=0.84, v_num=p0id] Epoch 4:  22% 28/125 [00:01<00:05, 17.68it/s, loss=0.84, v_num=p0id]Epoch 4:  22% 28/125 [00:01<00:05, 17.67it/s, loss=0.837, v_num=p0id]Epoch 4:  23% 29/125 [00:01<00:05, 17.82it/s, loss=0.838, v_num=p0id]Epoch 4:  24% 30/125 [00:01<00:05, 17.93it/s, loss=0.837, v_num=p0id]Epoch 4:  25% 31/125 [00:01<00:05, 18.11it/s, loss=0.837, v_num=p0id]Epoch 4:  26% 32/125 [00:01<00:05, 18.20it/s, loss=0.836, v_num=p0id]Epoch 4:  26% 33/125 [00:01<00:05, 18.28it/s, loss=0.836, v_num=p0id]Epoch 4:  27% 34/125 [00:01<00:04, 18.34it/s, loss=0.835, v_num=p0id]Epoch 4:  28% 35/125 [00:01<00:04, 18.42it/s, loss=0.835, v_num=p0id]Epoch 4:  28% 35/125 [00:01<00:04, 18.41it/s, loss=0.834, v_num=p0id]Epoch 4:  29% 36/125 [00:01<00:04, 18.54it/s, loss=0.835, v_num=p0id]Epoch 4:  30% 37/125 [00:01<00:04, 18.73it/s, loss=0.835, v_num=p0id]Epoch 4:  30% 38/125 [00:02<00:04, 18.66it/s, loss=0.839, v_num=p0id]Epoch 4:  31% 39/125 [00:02<00:04, 18.73it/s, loss=0.84, v_num=p0id] Epoch 4:  32% 40/125 [00:02<00:04, 18.88it/s, loss=0.843, v_num=p0id]Epoch 4:  33% 41/125 [00:02<00:04, 18.95it/s, loss=0.843, v_num=p0id]Epoch 4:  34% 42/125 [00:02<00:04, 19.03it/s, loss=0.843, v_num=p0id]Epoch 4:  34% 42/125 [00:02<00:04, 19.03it/s, loss=0.843, v_num=p0id]Epoch 4:  34% 43/125 [00:02<00:04, 19.09it/s, loss=0.843, v_num=p0id]Epoch 4:  35% 44/125 [00:02<00:04, 19.14it/s, loss=0.845, v_num=p0id]Epoch 4:  36% 45/125 [00:02<00:04, 19.19it/s, loss=0.845, v_num=p0id]Epoch 4:  37% 46/125 [00:02<00:04, 18.92it/s, loss=0.846, v_num=p0id]Epoch 4:  38% 47/125 [00:02<00:04, 19.02it/s, loss=0.845, v_num=p0id]Epoch 4:  38% 48/125 [00:02<00:04, 19.00it/s, loss=0.844, v_num=p0id]Epoch 4:  39% 49/125 [00:02<00:03, 19.02it/s, loss=0.844, v_num=p0id]Epoch 4:  39% 49/125 [00:02<00:03, 19.02it/s, loss=0.841, v_num=p0id]Epoch 4:  40% 50/125 [00:02<00:03, 19.06it/s, loss=0.841, v_num=p0id]Epoch 4:  41% 51/125 [00:02<00:03, 19.11it/s, loss=0.838, v_num=p0id]Epoch 4:  42% 52/125 [00:02<00:03, 19.09it/s, loss=0.837, v_num=p0id]Epoch 4:  42% 53/125 [00:02<00:03, 18.88it/s, loss=0.837, v_num=p0id]Epoch 4:  43% 54/125 [00:02<00:03, 18.96it/s, loss=0.838, v_num=p0id]Epoch 4:  44% 55/125 [00:02<00:03, 18.97it/s, loss=0.839, v_num=p0id]Epoch 4:  45% 56/125 [00:02<00:03, 19.02it/s, loss=0.839, v_num=p0id]Epoch 4:  45% 56/125 [00:02<00:03, 19.02it/s, loss=0.839, v_num=p0id]Epoch 4:  46% 57/125 [00:02<00:03, 19.12it/s, loss=0.839, v_num=p0id]Epoch 4:  46% 58/125 [00:03<00:03, 19.18it/s, loss=0.838, v_num=p0id]Epoch 4:  47% 59/125 [00:03<00:03, 19.11it/s, loss=0.836, v_num=p0id]Epoch 4:  48% 60/125 [00:03<00:03, 19.05it/s, loss=0.833, v_num=p0id]Epoch 4:  49% 61/125 [00:03<00:03, 18.94it/s, loss=0.832, v_num=p0id]Epoch 4:  50% 62/125 [00:03<00:03, 18.99it/s, loss=0.831, v_num=p0id]Epoch 4:  50% 63/125 [00:03<00:03, 19.06it/s, loss=0.831, v_num=p0id]Epoch 4:  50% 63/125 [00:03<00:03, 19.06it/s, loss=0.829, v_num=p0id]Epoch 4:  51% 64/125 [00:03<00:03, 19.10it/s, loss=0.827, v_num=p0id]Epoch 4:  52% 65/125 [00:03<00:03, 19.14it/s, loss=0.825, v_num=p0id]Epoch 4:  53% 66/125 [00:03<00:03, 19.18it/s, loss=0.823, v_num=p0id]Epoch 4:  54% 67/125 [00:03<00:03, 19.18it/s, loss=0.822, v_num=p0id]Epoch 4:  54% 68/125 [00:03<00:02, 19.21it/s, loss=0.822, v_num=p0id]Epoch 4:  55% 69/125 [00:03<00:02, 19.25it/s, loss=0.822, v_num=p0id]Epoch 4:  56% 70/125 [00:03<00:02, 19.16it/s, loss=0.822, v_num=p0id]Epoch 4:  56% 70/125 [00:03<00:02, 19.16it/s, loss=0.82, v_num=p0id] Epoch 4:  57% 71/125 [00:03<00:02, 18.98it/s, loss=0.82, v_num=p0id]Epoch 4:  58% 72/125 [00:03<00:02, 18.72it/s, loss=0.818, v_num=p0id]Epoch 4:  58% 73/125 [00:03<00:02, 18.73it/s, loss=0.816, v_num=p0id]Epoch 4:  59% 74/125 [00:03<00:02, 18.74it/s, loss=0.815, v_num=p0id]Epoch 4:  60% 75/125 [00:03<00:02, 18.76it/s, loss=0.815, v_num=p0id]Epoch 4:  61% 76/125 [00:04<00:02, 18.70it/s, loss=0.812, v_num=p0id]Epoch 4:  62% 77/125 [00:04<00:02, 18.72it/s, loss=0.812, v_num=p0id]Epoch 4:  62% 77/125 [00:04<00:02, 18.72it/s, loss=0.809, v_num=p0id]Epoch 4:  62% 78/125 [00:04<00:02, 18.72it/s, loss=0.808, v_num=p0id]Epoch 4:  63% 79/125 [00:04<00:02, 18.74it/s, loss=0.806, v_num=p0id]Epoch 4:  64% 80/125 [00:04<00:02, 17.71it/s, loss=0.806, v_num=p0id]Epoch 4:  65% 81/125 [00:04<00:02, 17.76it/s, loss=0.806, v_num=p0id]Epoch 4:  66% 82/125 [00:04<00:02, 17.82it/s, loss=0.808, v_num=p0id]Epoch 4:  66% 83/125 [00:04<00:02, 17.82it/s, loss=0.807, v_num=p0id]Epoch 4:  67% 84/125 [00:04<00:02, 17.82it/s, loss=0.807, v_num=p0id]Epoch 4:  67% 84/125 [00:04<00:02, 17.82it/s, loss=0.808, v_num=p0id]Epoch 4:  68% 85/125 [00:04<00:02, 17.86it/s, loss=0.809, v_num=p0id]Epoch 4:  69% 86/125 [00:04<00:02, 17.87it/s, loss=0.812, v_num=p0id]Epoch 4:  70% 87/125 [00:04<00:02, 17.91it/s, loss=0.812, v_num=p0id]Epoch 4:  70% 88/125 [00:04<00:02, 17.93it/s, loss=0.812, v_num=p0id]Epoch 4:  71% 89/125 [00:04<00:02, 17.95it/s, loss=0.813, v_num=p0id]Epoch 4:  72% 90/125 [00:05<00:01, 18.00it/s, loss=0.812, v_num=p0id]Epoch 4:  73% 91/125 [00:05<00:01, 17.27it/s, loss=0.812, v_num=p0id]Epoch 4:  73% 91/125 [00:05<00:01, 17.27it/s, loss=0.812, v_num=p0id]Epoch 4:  74% 92/125 [00:05<00:01, 17.25it/s, loss=0.811, v_num=p0id]Epoch 4:  74% 93/125 [00:05<00:01, 17.20it/s, loss=0.812, v_num=p0id]Epoch 4:  75% 94/125 [00:05<00:01, 17.20it/s, loss=0.816, v_num=p0id]Epoch 4:  76% 95/125 [00:05<00:01, 17.21it/s, loss=0.818, v_num=p0id]Epoch 4:  77% 96/125 [00:05<00:01, 17.25it/s, loss=0.821, v_num=p0id]Epoch 4:  78% 97/125 [00:05<00:01, 17.27it/s, loss=0.823, v_num=p0id]Epoch 4:  78% 98/125 [00:05<00:01, 17.01it/s, loss=0.823, v_num=p0id]Epoch 4:  78% 98/125 [00:05<00:01, 17.01it/s, loss=0.828, v_num=p0id]Epoch 4:  79% 99/125 [00:05<00:01, 17.01it/s, loss=0.829, v_num=p0id]Epoch 4:  80% 100/125 [00:05<00:01, 16.97it/s, loss=0.828, v_num=p0id]
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have rValidating: 0it [00:00, ?it/s][A
Validating:   0% 0/25 [00:00<?, ?it/s][Aequires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Epoch 4:  84% 105/125 [00:05<00:01, 17.59it/s, loss=0.828, v_num=p0id]
Validating:  20% 5/25 [00:00<00:00, 45.70it/s][A
Validating:  44% 11/25 [00:00<00:00, 48.34it/s][AEpoch 4:  90% 112/125 [00:06<00:00, 18.28it/s, loss=0.828, v_num=p0id]
Validating:  68% 17/25 [00:00<00:00, 50.21it/s][AEpoch 4:  95% 119/125 [00:06<00:00, 19.01it/s, loss=0.828, v_num=p0id]
Validating:  92% 23/25 [00:00<00:00, 52.01it/s][AEpoch 4: 100% 125/125 [00:06<00:00, 19.57it/s, loss=0.828, v_num=p0id]
                                               [AEpoch 4:   0% 0/125 [00:00<?, ?it/s, loss=0.828, v_num=p0id]          Epoch 5:   0% 0/125 [00:00<?, ?it/s, loss=0.828, v_num=p0id]Epoch 5:   1% 1/125 [00:00<00:06, 17.86it/s, loss=0.83, v_num=p0id]Epoch 5:   2% 2/125 [00:00<00:05, 20.76it/s, loss=0.831, v_num=p0id]Epoch 5:   2% 3/125 [00:00<00:06, 19.79it/s, loss=0.83, v_num=p0id] Epoch 5:   3% 4/125 [00:00<00:05, 21.41it/s, loss=0.833, v_num=p0id]Epoch 5:   4% 5/125 [00:00<00:05, 21.32it/s, loss=0.834, v_num=p0id]Epoch 5:   5% 6/125 [00:00<00:05, 20.37it/s, loss=0.835, v_num=p0id]Epoch 5:   6% 7/125 [00:00<00:05, 19.96it/s, loss=0.835, v_num=p0id]Epoch 5:   6% 7/125 [00:00<00:05, 19.95it/s, loss=0.835, v_num=p0id]Epoch 5:   6% 8/125 [00:00<00:05, 19.76it/s, loss=0.839, v_num=p0id]Epoch 5:   7% 9/125 [00:00<00:06, 17.70it/s, loss=0.838, v_num=p0id]Epoch 5:   8% 10/125 [00:00<00:09, 12.73it/s, loss=0.839, v_num=p0id]Epoch 5:   9% 11/125 [00:01<00:11,  9.97it/s, loss=0.84, v_num=p0id] Epoch 5:  10% 12/125 [00:01<00:13,  8.59it/s, loss=0.842, v_num=p0id]Epoch 5:  10% 13/125 [00:01<00:14,  7.62it/s, loss=0.84, v_num=p0id] Epoch 5:  11% 14/125 [00:01<00:14,  7.83it/s, loss=0.84, v_num=p0id]Epoch 5:  11% 14/125 [00:01<00:14,  7.83it/s, loss=0.834, v_num=p0id]Epoch 5:  12% 15/125 [00:01<00:13,  8.18it/s, loss=0.831, v_num=p0id]Epoch 5:  13% 16/125 [00:01<00:12,  8.51it/s, loss=0.826, v_num=p0id]Epoch 5:  14% 17/125 [00:01<00:12,  8.83it/s, loss=0.824, v_num=p0id]Epoch 5:  14% 18/125 [00:01<00:11,  9.13it/s, loss=0.816, v_num=p0id]Epoch 5:  15% 19/125 [00:02<00:11,  9.42it/s, loss=0.812, v_num=p0id]Epoch 5:  16% 20/125 [00:02<00:10,  9.68it/s, loss=0.809, v_num=p0id]Epoch 5:  17% 21/125 [00:02<00:10,  9.94it/s, loss=0.809, v_num=p0id]Epoch 5:  17% 21/125 [00:02<00:10,  9.94it/s, loss=0.802, v_num=p0id]Epoch 5:  18% 22/125 [00:02<00:10,  9.78it/s, loss=0.795, v_num=p0id]Epoch 5:  18% 23/125 [00:02<00:10, 10.04it/s, loss=0.793, v_num=p0id]Epoch 5:  19% 24/125 [00:02<00:09, 10.27it/s, loss=0.785, v_num=p0id]Epoch 5:  20% 25/125 [00:02<00:09, 10.51it/s, loss=0.78, v_num=p0id] Epoch 5:  21% 26/125 [00:02<00:09, 10.74it/s, loss=0.772, v_num=p0id]Epoch 5:  22% 27/125 [00:02<00:08, 10.96it/s, loss=0.768, v_num=p0id]Epoch 5:  22% 28/125 [00:02<00:08, 11.13it/s, loss=0.768, v_num=p0id]Epoch 5:  22% 28/125 [00:02<00:08, 11.13it/s, loss=0.759, v_num=p0id]Epoch 5:  23% 29/125 [00:02<00:08, 11.33it/s, loss=0.756, v_num=p0id]Epoch 5:  24% 30/125 [00:02<00:08, 11.52it/s, loss=0.752, v_num=p0id]Epoch 5:  25% 31/125 [00:02<00:08, 11.73it/s, loss=0.749, v_num=p0id]Epoch 5:  26% 32/125 [00:02<00:07, 11.90it/s, loss=0.743, v_num=p0id]Epoch 5:  26% 33/125 [00:02<00:07, 12.06it/s, loss=0.741, v_num=p0id]Epoch 5:  27% 34/125 [00:02<00:07, 12.21it/s, loss=0.737, v_num=p0id]Epoch 5:  28% 35/125 [00:02<00:07, 12.36it/s, loss=0.737, v_num=p0id]Epoch 5:  28% 35/125 [00:02<00:07, 12.36it/s, loss=0.733, v_num=p0id]Epoch 5:  29% 36/125 [00:02<00:07, 12.36it/s, loss=0.732, v_num=p0id]Epoch 5:  30% 37/125 [00:02<00:07, 12.57it/s, loss=0.728, v_num=p0id]Epoch 5:  30% 38/125 [00:02<00:06, 12.70it/s, loss=0.726, v_num=p0id]Epoch 5:  31% 39/125 [00:03<00:06, 12.84it/s, loss=0.723, v_num=p0id]Epoch 5:  32% 40/125 [00:03<00:06, 13.01it/s, loss=0.722, v_num=p0id]Epoch 5:  33% 41/125 [00:03<00:06, 13.16it/s, loss=0.72, v_num=p0id] Epoch 5:  34% 42/125 [00:03<00:06, 13.29it/s, loss=0.72, v_num=p0id]Epoch 5:  34% 42/125 [00:03<00:06, 13.29it/s, loss=0.717, v_num=p0id]Epoch 5:  34% 43/125 [00:03<00:06, 13.41it/s, loss=0.716, v_num=p0id]Epoch 5:  35% 44/125 [00:03<00:05, 13.53it/s, loss=0.713, v_num=p0id]Epoch 5:  36% 45/125 [00:03<00:05, 13.64it/s, loss=0.71, v_num=p0id] Epoch 5:  37% 46/125 [00:03<00:05, 13.75it/s, loss=0.71, v_num=p0id]Epoch 5:  38% 47/125 [00:03<00:05, 13.88it/s, loss=0.71, v_num=p0id]Epoch 5:  38% 48/125 [00:03<00:05, 13.95it/s, loss=0.71, v_num=p0id]Epoch 5:  39% 49/125 [00:03<00:05, 14.04it/s, loss=0.71, v_num=p0id]Epoch 5:  39% 49/125 [00:03<00:05, 14.04it/s, loss=0.707, v_num=p0id]Epoch 5:  40% 50/125 [00:03<00:05, 14.14it/s, loss=0.706, v_num=p0id]Epoch 5:  41% 51/125 [00:03<00:05, 14.23it/s, loss=0.702, v_num=p0id]Epoch 5:  42% 52/125 [00:03<00:05, 14.35it/s, loss=0.706, v_num=p0id]Epoch 5:  42% 53/125 [00:03<00:05, 13.74it/s, loss=0.713, v_num=p0id]Epoch 5:  43% 54/125 [00:03<00:05, 13.64it/s, loss=0.722, v_num=p0id]Epoch 5:  44% 55/125 [00:04<00:05, 13.61it/s, loss=0.729, v_num=p0id]Epoch 5:  45% 56/125 [00:04<00:05, 13.71it/s, loss=0.729, v_num=p0id]Epoch 5:  45% 56/125 [00:04<00:05, 13.71it/s, loss=0.737, v_num=p0id]Epoch 5:  46% 57/125 [00:04<00:04, 13.77it/s, loss=0.748, v_num=p0id]Epoch 5:  46% 58/125 [00:04<00:04, 13.83it/s, loss=0.76, v_num=p0id] Epoch 5:  47% 59/125 [00:04<00:04, 13.90it/s, loss=0.772, v_num=p0id]Epoch 5:  48% 60/125 [00:04<00:04, 13.97it/s, loss=0.781, v_num=p0id]Epoch 5:  49% 61/125 [00:04<00:04, 14.06it/s, loss=0.794, v_num=p0id]Epoch 5:  50% 62/125 [00:04<00:04, 14.14it/s, loss=0.805, v_num=p0id]Epoch 5:  50% 63/125 [00:04<00:04, 14.22it/s, loss=0.805, v_num=p0id]Epoch 5:  50% 63/125 [00:04<00:04, 14.22it/s, loss=0.814, v_num=p0id]Epoch 5:  51% 64/125 [00:04<00:04, 14.30it/s, loss=0.825, v_num=p0id]Epoch 5:  52% 65/125 [00:04<00:04, 14.37it/s, loss=0.835, v_num=p0id]Epoch 5:  53% 66/125 [00:04<00:04, 14.45it/s, loss=0.842, v_num=p0id]Epoch 5:  54% 67/125 [00:04<00:03, 14.50it/s, loss=0.848, v_num=p0id]Epoch 5:  54% 68/125 [00:04<00:03, 14.57it/s, loss=0.857, v_num=p0id]Epoch 5:  55% 69/125 [00:04<00:03, 14.64it/s, loss=0.865, v_num=p0id]Epoch 5:  56% 70/125 [00:04<00:03, 14.65it/s, loss=0.865, v_num=p0id]Epoch 5:  56% 70/125 [00:04<00:03, 14.65it/s, loss=0.873, v_num=p0id]Epoch 5:  57% 71/125 [00:04<00:03, 14.66it/s, loss=0.881, v_num=p0id]Epoch 5:  58% 72/125 [00:04<00:03, 14.50it/s, loss=0.883, v_num=p0id]Epoch 5:  58% 73/125 [00:05<00:03, 14.40it/s, loss=0.881, v_num=p0id]Epoch 5:  59% 74/125 [00:05<00:03, 14.45it/s, loss=0.88, v_num=p0id] Epoch 5:  60% 75/125 [00:05<00:03, 14.51it/s, loss=0.879, v_num=p0id]Epoch 5:  61% 76/125 [00:05<00:03, 14.56it/s, loss=0.874, v_num=p0id]Epoch 5:  62% 77/125 [00:05<00:03, 14.47it/s, loss=0.874, v_num=p0id]Epoch 5:  62% 77/125 [00:05<00:03, 14.47it/s, loss=0.869, v_num=p0id]Epoch 5:  62% 78/125 [00:05<00:03, 14.52it/s, loss=0.862, v_num=p0id]Epoch 5:  63% 79/125 [00:05<00:03, 14.57it/s, loss=0.856, v_num=p0id]Epoch 5:  64% 80/125 [00:05<00:03, 14.60it/s, loss=0.849, v_num=p0id]Epoch 5:  65% 81/125 [00:05<00:03, 14.67it/s, loss=0.842, v_num=p0id]Epoch 5:  66% 82/125 [00:05<00:02, 14.73it/s, loss=0.834, v_num=p0id]Epoch 5:  66% 83/125 [00:05<00:02, 14.77it/s, loss=0.826, v_num=p0id]Epoch 5:  67% 84/125 [00:05<00:02, 14.80it/s, loss=0.826, v_num=p0id]Epoch 5:  67% 84/125 [00:05<00:02, 14.80it/s, loss=0.818, v_num=p0id]Epoch 5:  68% 85/125 [00:05<00:02, 14.86it/s, loss=0.813, v_num=p0id]Epoch 5:  69% 86/125 [00:05<00:02, 14.89it/s, loss=0.807, v_num=p0id]Epoch 5:  70% 87/125 [00:05<00:02, 14.95it/s, loss=0.8, v_num=p0id]  Epoch 5:  70% 88/125 [00:05<00:02, 14.99it/s, loss=0.792, v_num=p0id]Epoch 5:  71% 89/125 [00:05<00:02, 15.03it/s, loss=0.788, v_num=p0id]Epoch 5:  72% 90/125 [00:05<00:02, 15.09it/s, loss=0.78, v_num=p0id] Epoch 5:  73% 91/125 [00:06<00:02, 15.12it/s, loss=0.78, v_num=p0id]Epoch 5:  73% 91/125 [00:06<00:02, 15.12it/s, loss=0.777, v_num=p0id]Epoch 5:  74% 92/125 [00:06<00:02, 15.12it/s, loss=0.77, v_num=p0id] Epoch 5:  74% 93/125 [00:06<00:02, 15.16it/s, loss=0.764, v_num=p0id]Epoch 5:  75% 94/125 [00:06<00:02, 15.21it/s, loss=0.758, v_num=p0id]Epoch 5:  76% 95/125 [00:06<00:01, 15.27it/s, loss=0.75, v_num=p0id] Epoch 5:  77% 96/125 [00:06<00:01, 15.32it/s, loss=0.746, v_num=p0id]Epoch 5:  78% 97/125 [00:06<00:01, 15.35it/s, loss=0.74, v_num=p0id] Epoch 5:  78% 98/125 [00:06<00:01, 15.42it/s, loss=0.74, v_num=p0id]Epoch 5:  78% 98/125 [00:06<00:01, 15.42it/s, loss=0.738, v_num=p0id]Epoch 5:  79% 99/125 [00:06<00:01, 15.40it/s, loss=0.733, v_num=p0id]Epoch 5:  80% 100/125 [00:06<00:01, 15.38it/s, loss=0.729, v_num=p0id]
Validating: 0it [00:00, ?it/s][A/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")

Validating:   0% 0/25 [00:00<?, ?it/s][AEpoch 5:  84% 105/125 [00:06<00:01, 15.95it/s, loss=0.729, v_num=p0id]
Validating:  24% 6/25 [00:00<00:00, 46.43it/s][AEpoch 5:  90% 112/125 [00:06<00:00, 16.63it/s, loss=0.729, v_num=p0id]
Validating:  48% 12/25 [00:00<00:00, 48.64it/s][A
Validating:  72% 18/25 [00:00<00:00, 51.60it/s][AEpoch 5:  95% 119/125 [00:06<00:00, 17.34it/s, loss=0.729, v_num=p0id]
Validating:  96% 24/25 [00:00<00:00, 53.66it/s][AEpoch 5: 100% 125/125 [00:06<00:00, 17.89it/s, loss=0.729, v_num=p0id]
                                               [AEpoch 5:   0% 0/125 [00:00<?, ?it/s, loss=0.729, v_num=p0id]          Epoch 6:   0% 0/125 [00:00<?, ?it/s, loss=0.729, v_num=p0id]Epoch 6:   1% 1/125 [00:00<00:07, 17.68it/s, loss=0.726, v_num=p0id]Epoch 6:   2% 2/125 [00:00<00:05, 20.68it/s, loss=0.725, v_num=p0id]Epoch 6:   2% 3/125 [00:00<00:06, 19.66it/s, loss=0.723, v_num=p0id]Epoch 6:   3% 4/125 [00:00<00:05, 20.89it/s, loss=0.721, v_num=p0id]Epoch 6:   4% 5/125 [00:00<00:05, 20.77it/s, loss=0.717, v_num=p0id]Epoch 6:   5% 6/125 [00:00<00:05, 20.86it/s, loss=0.713, v_num=p0id]Epoch 6:   6% 7/125 [00:00<00:05, 20.37it/s, loss=0.713, v_num=p0id]Epoch 6:   6% 7/125 [00:00<00:05, 20.35it/s, loss=0.712, v_num=p0id]Epoch 6:   6% 8/125 [00:00<00:05, 20.71it/s, loss=0.713, v_num=p0id]Epoch 6:   7% 9/125 [00:00<00:05, 20.80it/s, loss=0.708, v_num=p0id]Epoch 6:   8% 10/125 [00:00<00:05, 20.79it/s, loss=0.708, v_num=p0id]Epoch 6:   9% 11/125 [00:00<00:05, 20.63it/s, loss=0.706, v_num=p0id]Epoch 6:  10% 12/125 [00:00<00:05, 20.56it/s, loss=0.707, v_num=p0id]Epoch 6:  10% 13/125 [00:00<00:05, 20.55it/s, loss=0.704, v_num=p0id]Epoch 6:  11% 14/125 [00:00<00:05, 20.32it/s, loss=0.704, v_num=p0id]Epoch 6:  11% 14/125 [00:00<00:05, 20.32it/s, loss=0.702, v_num=p0id]Epoch 6:  12% 15/125 [00:00<00:05, 20.41it/s, loss=0.7, v_num=p0id]  Epoch 6:  13% 16/125 [00:00<00:05, 20.48it/s, loss=0.699, v_num=p0id]Epoch 6:  14% 17/125 [00:00<00:05, 20.59it/s, loss=0.699, v_num=p0id]Epoch 6:  14% 18/125 [00:00<00:05, 20.61it/s, loss=0.693, v_num=p0id]Epoch 6:  15% 19/125 [00:00<00:05, 20.70it/s, loss=0.691, v_num=p0id]Epoch 6:  16% 20/125 [00:00<00:05, 20.67it/s, loss=0.691, v_num=p0id]Epoch 6:  17% 21/125 [00:01<00:05, 20.71it/s, loss=0.691, v_num=p0id]Epoch 6:  17% 21/125 [00:01<00:05, 20.70it/s, loss=0.687, v_num=p0id]Epoch 6:  18% 22/125 [00:01<00:04, 20.64it/s, loss=0.684, v_num=p0id]Epoch 6:  18% 23/125 [00:01<00:04, 20.76it/s, loss=0.683, v_num=p0id]Epoch 6:  19% 24/125 [00:01<00:04, 20.76it/s, loss=0.683, v_num=p0id]Epoch 6:  20% 25/125 [00:01<00:04, 20.86it/s, loss=0.683, v_num=p0id]Epoch 6:  21% 26/125 [00:01<00:04, 20.96it/s, loss=0.683, v_num=p0id]Epoch 6:  22% 27/125 [00:01<00:04, 21.03it/s, loss=0.682, v_num=p0id]Epoch 6:  22% 28/125 [00:01<00:04, 20.97it/s, loss=0.682, v_num=p0id]Epoch 6:  22% 28/125 [00:01<00:04, 20.97it/s, loss=0.678, v_num=p0id]Epoch 6:  23% 29/125 [00:01<00:04, 21.04it/s, loss=0.68, v_num=p0id] Epoch 6:  24% 30/125 [00:01<00:04, 21.07it/s, loss=0.678, v_num=p0id]Epoch 6:  25% 31/125 [00:01<00:04, 21.20it/s, loss=0.678, v_num=p0id]Epoch 6:  26% 32/125 [00:01<00:04, 21.05it/s, loss=0.675, v_num=p0id]Epoch 6:  26% 33/125 [00:01<00:04, 21.02it/s, loss=0.676, v_num=p0id]Epoch 6:  27% 34/125 [00:01<00:04, 18.89it/s, loss=0.675, v_num=p0id]Epoch 6:  28% 35/125 [00:01<00:04, 18.95it/s, loss=0.675, v_num=p0id]Epoch 6:  28% 35/125 [00:01<00:04, 18.94it/s, loss=0.675, v_num=p0id]Epoch 6:  29% 36/125 [00:01<00:04, 19.07it/s, loss=0.675, v_num=p0id]Epoch 6:  30% 37/125 [00:01<00:04, 19.27it/s, loss=0.673, v_num=p0id]Epoch 6:  30% 38/125 [00:01<00:04, 19.31it/s, loss=0.674, v_num=p0id]Epoch 6:  31% 39/125 [00:02<00:04, 19.37it/s, loss=0.675, v_num=p0id]Epoch 6:  32% 40/125 [00:02<00:04, 19.51it/s, loss=0.675, v_num=p0id]Epoch 6:  33% 41/125 [00:02<00:04, 19.60it/s, loss=0.675, v_num=p0id]Epoch 6:  34% 42/125 [00:02<00:04, 19.67it/s, loss=0.675, v_num=p0id]Epoch 6:  34% 42/125 [00:02<00:04, 19.67it/s, loss=0.675, v_num=p0id]Epoch 6:  34% 43/125 [00:02<00:04, 19.69it/s, loss=0.675, v_num=p0id]Epoch 6:  35% 44/125 [00:02<00:04, 19.51it/s, loss=0.675, v_num=p0id]Epoch 6:  36% 45/125 [00:02<00:04, 19.43it/s, loss=0.674, v_num=p0id]Epoch 6:  37% 46/125 [00:02<00:04, 19.47it/s, loss=0.675, v_num=p0id]Epoch 6:  38% 47/125 [00:02<00:03, 19.56it/s, loss=0.675, v_num=p0id]Epoch 6:  38% 48/125 [00:02<00:03, 19.48it/s, loss=0.675, v_num=p0id]Epoch 6:  39% 49/125 [00:02<00:03, 19.49it/s, loss=0.675, v_num=p0id]Epoch 6:  39% 49/125 [00:02<00:03, 19.49it/s, loss=0.674, v_num=p0id]Epoch 6:  40% 50/125 [00:02<00:03, 19.53it/s, loss=0.674, v_num=p0id]Epoch 6:  41% 51/125 [00:02<00:03, 19.57it/s, loss=0.673, v_num=p0id]Epoch 6:  42% 52/125 [00:02<00:03, 19.64it/s, loss=0.674, v_num=p0id]Epoch 6:  42% 53/125 [00:02<00:03, 19.39it/s, loss=0.673, v_num=p0id]Epoch 6:  43% 54/125 [00:02<00:03, 19.44it/s, loss=0.673, v_num=p0id]Epoch 6:  44% 55/125 [00:02<00:03, 19.45it/s, loss=0.674, v_num=p0id]Epoch 6:  45% 56/125 [00:02<00:03, 19.49it/s, loss=0.674, v_num=p0id]Epoch 6:  45% 56/125 [00:02<00:03, 19.49it/s, loss=0.674, v_num=p0id]Epoch 6:  46% 57/125 [00:02<00:03, 19.59it/s, loss=0.676, v_num=p0id]Epoch 6:  46% 58/125 [00:02<00:03, 19.65it/s, loss=0.678, v_num=p0id]Epoch 6:  47% 59/125 [00:03<00:03, 19.66it/s, loss=0.677, v_num=p0id]Epoch 6:  48% 60/125 [00:03<00:03, 19.67it/s, loss=0.677, v_num=p0id]Epoch 6:  49% 61/125 [00:03<00:03, 19.70it/s, loss=0.678, v_num=p0id]Epoch 6:  50% 62/125 [00:03<00:03, 19.74it/s, loss=0.68, v_num=p0id] Epoch 6:  50% 63/125 [00:03<00:03, 19.79it/s, loss=0.68, v_num=p0id]Epoch 6:  50% 63/125 [00:03<00:03, 19.79it/s, loss=0.675, v_num=p0id]Epoch 6:  51% 64/125 [00:03<00:03, 19.40it/s, loss=0.677, v_num=p0id]Epoch 6:  52% 65/125 [00:03<00:03, 18.79it/s, loss=0.677, v_num=p0id]Epoch 6:  53% 66/125 [00:03<00:03, 18.83it/s, loss=0.676, v_num=p0id]Epoch 6:  54% 67/125 [00:03<00:03, 18.83it/s, loss=0.674, v_num=p0id]Epoch 6:  54% 68/125 [00:03<00:03, 18.70it/s, loss=0.673, v_num=p0id]Epoch 6:  55% 69/125 [00:03<00:02, 18.75it/s, loss=0.672, v_num=p0id]Epoch 6:  56% 70/125 [00:03<00:02, 18.52it/s, loss=0.672, v_num=p0id]Epoch 6:  56% 70/125 [00:03<00:02, 18.52it/s, loss=0.672, v_num=p0id]Epoch 6:  57% 71/125 [00:03<00:02, 18.35it/s, loss=0.671, v_num=p0id]Epoch 6:  58% 72/125 [00:03<00:02, 18.35it/s, loss=0.668, v_num=p0id]Epoch 6:  58% 73/125 [00:03<00:02, 18.37it/s, loss=0.669, v_num=p0id]Epoch 6:  59% 74/125 [00:04<00:02, 18.37it/s, loss=0.67, v_num=p0id] Epoch 6:  60% 75/125 [00:04<00:02, 18.41it/s, loss=0.671, v_num=p0id]Epoch 6:  61% 76/125 [00:04<00:02, 18.41it/s, loss=0.669, v_num=p0id]Epoch 6:  62% 77/125 [00:04<00:02, 18.44it/s, loss=0.669, v_num=p0id]Epoch 6:  62% 77/125 [00:04<00:02, 18.44it/s, loss=0.668, v_num=p0id]Epoch 6:  62% 78/125 [00:04<00:02, 18.43it/s, loss=0.667, v_num=p0id]Epoch 6:  63% 79/125 [00:04<00:02, 18.45it/s, loss=0.667, v_num=p0id]Epoch 6:  64% 80/125 [00:04<00:02, 18.44it/s, loss=0.665, v_num=p0id]Epoch 6:  65% 81/125 [00:04<00:02, 18.49it/s, loss=0.666, v_num=p0id]Epoch 6:  66% 82/125 [00:04<00:02, 18.54it/s, loss=0.663, v_num=p0id]Epoch 6:  66% 83/125 [00:04<00:02, 18.53it/s, loss=0.664, v_num=p0id]Epoch 6:  67% 84/125 [00:04<00:02, 18.52it/s, loss=0.664, v_num=p0id]Epoch 6:  67% 84/125 [00:04<00:02, 18.52it/s, loss=0.662, v_num=p0id]Epoch 6:  68% 85/125 [00:04<00:02, 18.55it/s, loss=0.661, v_num=p0id]Epoch 6:  69% 86/125 [00:04<00:02, 18.55it/s, loss=0.661, v_num=p0id]Epoch 6:  70% 87/125 [00:04<00:02, 18.59it/s, loss=0.66, v_num=p0id] Epoch 6:  70% 88/125 [00:04<00:01, 18.59it/s, loss=0.661, v_num=p0id]Epoch 6:  71% 89/125 [00:04<00:01, 18.61it/s, loss=0.663, v_num=p0id]Epoch 6:  72% 90/125 [00:04<00:01, 18.65it/s, loss=0.66, v_num=p0id] Epoch 6:  73% 91/125 [00:04<00:01, 18.64it/s, loss=0.66, v_num=p0id]Epoch 6:  73% 91/125 [00:04<00:01, 18.64it/s, loss=0.661, v_num=p0id]Epoch 6:  74% 92/125 [00:04<00:01, 18.60it/s, loss=0.661, v_num=p0id]Epoch 6:  74% 93/125 [00:04<00:01, 18.60it/s, loss=0.66, v_num=p0id] Epoch 6:  75% 94/125 [00:05<00:01, 18.65it/s, loss=0.658, v_num=p0id]Epoch 6:  76% 95/125 [00:05<00:01, 18.68it/s, loss=0.655, v_num=p0id]Epoch 6:  77% 96/125 [00:05<00:01, 18.72it/s, loss=0.655, v_num=p0id]Epoch 6:  78% 97/125 [00:05<00:01, 18.73it/s, loss=0.655, v_num=p0id]Epoch 6:  78% 98/125 [00:05<00:01, 18.77it/s, loss=0.655, v_num=p0id]Epoch 6:  78% 98/125 [00:05<00:01, 18.77it/s, loss=0.655, v_num=p0id]Epoch 6:  79% 99/125 [00:05<00:01, 18.27it/s, loss=0.655, v_num=p0id]Epoch 6:  80% 100/125 [00:05<00:01, 18.19it/s, loss=0.656, v_num=p0id]/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of t
Validating: 0it [00:00, ?it/s][A
Validating:   0% 0/25 [00:00<?, ?it/s][Ahe inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/global/homes/d/danieltm/.conda/envs/exatrkx-test/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Epoch 6:  84% 105/125 [00:05<00:01, 18.83it/s, loss=0.656, v_num=p0id]
Validating:  24% 6/25 [00:00<00:00, 31.48it/s][A
Validating:  44% 11/25 [00:00<00:00, 39.41it/s][AEpoch 6:  90% 112/125 [00:05<00:00, 19.34it/s, loss=0.656, v_num=p0id]srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 2590767 ON cgpu01 CANCELLED AT 2022-01-10T23:30:15 ***
slurmstepd: error: *** STEP 2590767.0 ON cgpu01 CANCELLED AT 2022-01-10T23:30:15 ***
